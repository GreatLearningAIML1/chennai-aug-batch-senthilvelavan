{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Wine Quality Prediction-\n",
    "Here, we will apply a method of assessing wine quality using a decision tree, and test it against the wine-quality dataset from the UC Irvine Machine Learning Repository.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wine dataset is a classic and very easy multi-class classification dataset.\n",
    "Find more about data here - https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the necessary modules\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer  #DT does not take strings as input for the model fit step...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. Load the Wine Data file into Python DataFrame. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume the data frame is named wine_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df = pd.read_csv('wine.csv', names = [\"Cultivator\", \"Alchol\", \"Malic_Acid\", \"Ash\", \"Alcalinity_of_Ash\", \"Magnesium\", \"Total_phenols\", \"Falvanoids\", \"Nonflavanoid_phenols\", \"Proanthocyanins\", \"Color_intensity\", \"Hue\", \"OD280\", \"Proline\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always a good practice to eye-ball raw data to get a feel of the data in terms of number of structure of the file, number of attributes, types of attributes and a general idea of likely challenges in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. Print 10 samples from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 Print the datatypes of each column and the shape of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4 Print the descriptive statistics of each & every column using describe() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df.describe().transpose()\n",
    "wine_df.info() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5 Using univariate analysis check the individual attributes for their basic statistic such as central values, spread, tails etc. What are your observations (any two attributes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the graphs of different variable to see the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for example - \n",
    "#Sulfur.dioxide (both Free and Total) is distributed over a wide range across the samples.\n",
    "#The alcohol content varies from 8.40 to 14.90.\n",
    "#The quality of the samples range from 3 to 8 with 6 being the median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6. Use pairplots and correlation method to observe the relationship between different variables and state your insights.\n",
    "Hint: Use seaborn plot and check the relationship between different variables \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df.corr()\n",
    "\n",
    "sns.pairplot(wine_df, hue=\"Class label\", size=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7 Split the wine_df into training and test set in the ratio of 70:30 (Training:Test) based on dependent and independent variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into training and test set for independent attributes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X =  wine_df.drop(\"Cultivator\", axis=1)\n",
    "y =  wine_df.pop(\"Cultivator\")\n",
    "\n",
    "train_set, test_set, train_labels, test_labels = train_test_split(X, y, test_size=0.30, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8 Create the decision tree model using “entropy” method of finding the split columns and fit it to training data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoking the decision tree classifier function. Using 'entropy' method of finding the split columns. Other option \n",
    "# could be gini index.  Restricting the depth of the tree to 5 (no particular reason for selecting this)\n",
    "\n",
    "#dt_model = DecisionTreeClassifier(criterion = 'entropy' , max_depth = 5, random_state = 100)\n",
    "                                  \n",
    "dt_model = DecisionTreeClassifier(criterion = 'entropy' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model.fit(train_set, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q9 Print the accuracy of the model & print the confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model.score(test_set , test_labels)  # performance on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q10. Regularize the decision tree by limiting the max. depth of trees and print the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = DecisionTreeClassifier(criterion = 'entropy', max_depth = 3)\n",
    "dt_model.fit(train_set, train_labels)\n",
    "test_pred = dt_model.predict(test_set)\n",
    "dt_model.score(test_set , test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q11 Apply the Random forest model and print the accuracy of Random forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfcl = RandomForestClassifier(n_estimators = 50)\n",
    "rfcl = rfcl.fit(train_set, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = rfcl.predict(test_set)\n",
    "rfcl.score(test_set , test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q12 Apply Adaboost Ensemble Algorithm for the same data and print the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "#abcl = AdaBoostClassifier(base_estimator=dt_model, n_estimators=50)\n",
    "abcl = AdaBoostClassifier( n_estimators= 20)\n",
    "abcl = abcl.fit(train_set, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = abcl.predict(test_set)\n",
    "abcl.score(test_set , test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q13 - Apply Bagging Classifier Algorithm and print the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bgcl = BaggingClassifier(n_estimators=10, max_samples= .7, bootstrap=True, oob_score=True)\n",
    "bgcl = bgcl.fit(train_set, train_labels)\n",
    "bgcl.oob_score_     ### testing the ensemble against OOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = bgcl.predict(test_set)\n",
    "bgcl.score(test_set , test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q14 - Apply GradientBoost Classifier Algorithm for the same data and print the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbcl = GradientBoostingClassifier(n_estimators = 50, learning_rate = 0.05)\n",
    "gbcl = gbcl.fit(train_set, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = gbcl.predict(test_set)\n",
    "gbcl.score(test_set , test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps are optional and will not be graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting is one of the simplest ways of combining the predictions from multiple machine learning algorithms.\n",
    "\n",
    "It works by first creating two or more standalone models from your training dataset. A Voting Classifier can then be used to wrap your models and average the predictions of the sub-models when asked to make predictions for new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Voting Classfier on the given dataset and state your insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Comparision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into training and test set for independent attributes\n",
    "\n",
    "wine_df = pd.read_csv('wine.csv', names = [\"Cultivator\", \"Alchol\", \"Malic_Acid\", \"Ash\", \"Alcalinity_of_Ash\", \"Magnesium\", \"Total_phenols\", \"Falvanoids\", \"Nonflavanoid_phenols\", \"Proanthocyanins\", \"Color_intensity\", \"Hue\", \"OD280\", \"Proline\"])\n",
    "X = wine_df.drop(\"Cultivator\", axis=1)\n",
    "y = wine_df.pop(\"Cultivator\")\n",
    "\n",
    "\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "\tkfold = model_selection.KFold(n_splits=10, random_state=12345)\n",
    "\tcv_results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n",
    "\tresults.append(cv_results)\n",
    "\tnames.append(name)\n",
    "\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "\tprint(msg)\n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DeepLearning_R6_Project1.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"QH-7TQ0iQgmp","colab_type":"code","colab":{}},"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NkSiGxZrbYNI","colab_type":"text"},"cell_type":"markdown","source":["<h3>Load bank data</h3>"]},{"metadata":{"id":"aCw6p6AeQ116","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"5fba3734-722d-4908-e04a-a65761504686","executionInfo":{"status":"ok","timestamp":1549915065076,"user_tz":-330,"elapsed":24037,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive',force_remount=True)"],"execution_count":68,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"-HFj_H7aRDPx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":215},"outputId":"2f126a43-8859-4450-b9dc-8bff09fe28f5","executionInfo":{"status":"ok","timestamp":1549915065077,"user_tz":-330,"elapsed":24011,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["bank_df = pd.read_csv('/content/gdrive/My Drive/AIML/Projects/DeepLearning/bank.csv')\n","bank_df.head()"],"execution_count":69,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>RowNumber</th>\n","      <th>CustomerId</th>\n","      <th>Surname</th>\n","      <th>CreditScore</th>\n","      <th>Geography</th>\n","      <th>Gender</th>\n","      <th>Age</th>\n","      <th>Tenure</th>\n","      <th>Balance</th>\n","      <th>NumOfProducts</th>\n","      <th>HasCrCard</th>\n","      <th>IsActiveMember</th>\n","      <th>EstimatedSalary</th>\n","      <th>Exited</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>15634602</td>\n","      <td>Hargrave</td>\n","      <td>619</td>\n","      <td>France</td>\n","      <td>Female</td>\n","      <td>42</td>\n","      <td>2</td>\n","      <td>0.00</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>101348.88</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>15647311</td>\n","      <td>Hill</td>\n","      <td>608</td>\n","      <td>Spain</td>\n","      <td>Female</td>\n","      <td>41</td>\n","      <td>1</td>\n","      <td>83807.86</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>112542.58</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>15619304</td>\n","      <td>Onio</td>\n","      <td>502</td>\n","      <td>France</td>\n","      <td>Female</td>\n","      <td>42</td>\n","      <td>8</td>\n","      <td>159660.80</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>113931.57</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>15701354</td>\n","      <td>Boni</td>\n","      <td>699</td>\n","      <td>France</td>\n","      <td>Female</td>\n","      <td>39</td>\n","      <td>1</td>\n","      <td>0.00</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>93826.63</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>15737888</td>\n","      <td>Mitchell</td>\n","      <td>850</td>\n","      <td>Spain</td>\n","      <td>Female</td>\n","      <td>43</td>\n","      <td>2</td>\n","      <td>125510.82</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>79084.10</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n","0          1    15634602  Hargrave          619    France  Female   42   \n","1          2    15647311      Hill          608     Spain  Female   41   \n","2          3    15619304      Onio          502    France  Female   42   \n","3          4    15701354      Boni          699    France  Female   39   \n","4          5    15737888  Mitchell          850     Spain  Female   43   \n","\n","   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n","0       2       0.00              1          1               1   \n","1       1   83807.86              1          0               1   \n","2       8  159660.80              3          1               0   \n","3       1       0.00              2          0               0   \n","4       2  125510.82              1          1               1   \n","\n","   EstimatedSalary  Exited  \n","0        101348.88       1  \n","1        112542.58       0  \n","2        113931.57       1  \n","3         93826.63       0  \n","4         79084.10       0  "]},"metadata":{"tags":[]},"execution_count":69}]},{"metadata":{"id":"Jm3SVYMzRiZa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":351},"outputId":"15e32e30-ff42-49a6-ec47-055a0499f5ef","executionInfo":{"status":"ok","timestamp":1549915065078,"user_tz":-330,"elapsed":23995,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["bank_df.info()"],"execution_count":70,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 10000 entries, 0 to 9999\n","Data columns (total 14 columns):\n","RowNumber          10000 non-null int64\n","CustomerId         10000 non-null int64\n","Surname            10000 non-null object\n","CreditScore        10000 non-null int64\n","Geography          10000 non-null object\n","Gender             10000 non-null object\n","Age                10000 non-null int64\n","Tenure             10000 non-null int64\n","Balance            10000 non-null float64\n","NumOfProducts      10000 non-null int64\n","HasCrCard          10000 non-null int64\n","IsActiveMember     10000 non-null int64\n","EstimatedSalary    10000 non-null float64\n","Exited             10000 non-null int64\n","dtypes: float64(2), int64(9), object(3)\n","memory usage: 1.1+ MB\n"],"name":"stdout"}]},{"metadata":{"id":"NgWObDLsbdcm","colab_type":"text"},"cell_type":"markdown","source":["<h3>Analysis of data</h3>\n","\n","To find out whether some of the columns like CustomerId and RowNumber are significant, finding out the value counts for the unique values as below. Seems like these two columns can be removed\n","\n","\n","*   CustomerId\n","*   RowNumber\n","\n","There are 3 non-numeric columns, which can be label encoded and those are listed below:\n","\n","*   Surname\n","*   Geography\n","*   Gender\n","\n","\n","\n","\n","\n","\n","---\n","\n"]},{"metadata":{"id":"8yq-6FqdVd9c","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"3107437b-f942-466f-b3ca-168fe94bde1f","executionInfo":{"status":"ok","timestamp":1549915065078,"user_tz":-330,"elapsed":23978,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["bank_df['CustomerId'].value_counts().unique()"],"execution_count":71,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1])"]},"metadata":{"tags":[]},"execution_count":71}]},{"metadata":{"id":"yLn1ja29VjuO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"1d8ee92e-ed74-48cd-92aa-921e1ba7356c","executionInfo":{"status":"ok","timestamp":1549915065085,"user_tz":-330,"elapsed":23968,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["bank_df['RowNumber'].value_counts().unique()"],"execution_count":72,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1])"]},"metadata":{"tags":[]},"execution_count":72}]},{"metadata":{"id":"sAkbC4ZiVyAh","colab_type":"code","colab":{}},"cell_type":"code","source":["bank_df.drop(columns=['RowNumber','CustomerId'], axis=1, inplace=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"teoDO-_BV_dP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":195},"outputId":"56791b88-2927-47f4-fd48-d59135798ca6","executionInfo":{"status":"ok","timestamp":1549915065089,"user_tz":-330,"elapsed":23940,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["bank_df.head()"],"execution_count":74,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Surname</th>\n","      <th>CreditScore</th>\n","      <th>Geography</th>\n","      <th>Gender</th>\n","      <th>Age</th>\n","      <th>Tenure</th>\n","      <th>Balance</th>\n","      <th>NumOfProducts</th>\n","      <th>HasCrCard</th>\n","      <th>IsActiveMember</th>\n","      <th>EstimatedSalary</th>\n","      <th>Exited</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Hargrave</td>\n","      <td>619</td>\n","      <td>France</td>\n","      <td>Female</td>\n","      <td>42</td>\n","      <td>2</td>\n","      <td>0.00</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>101348.88</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Hill</td>\n","      <td>608</td>\n","      <td>Spain</td>\n","      <td>Female</td>\n","      <td>41</td>\n","      <td>1</td>\n","      <td>83807.86</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>112542.58</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Onio</td>\n","      <td>502</td>\n","      <td>France</td>\n","      <td>Female</td>\n","      <td>42</td>\n","      <td>8</td>\n","      <td>159660.80</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>113931.57</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Boni</td>\n","      <td>699</td>\n","      <td>France</td>\n","      <td>Female</td>\n","      <td>39</td>\n","      <td>1</td>\n","      <td>0.00</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>93826.63</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Mitchell</td>\n","      <td>850</td>\n","      <td>Spain</td>\n","      <td>Female</td>\n","      <td>43</td>\n","      <td>2</td>\n","      <td>125510.82</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>79084.10</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Surname  CreditScore Geography  Gender  Age  Tenure    Balance  \\\n","0  Hargrave          619    France  Female   42       2       0.00   \n","1      Hill          608     Spain  Female   41       1   83807.86   \n","2      Onio          502    France  Female   42       8  159660.80   \n","3      Boni          699    France  Female   39       1       0.00   \n","4  Mitchell          850     Spain  Female   43       2  125510.82   \n","\n","   NumOfProducts  HasCrCard  IsActiveMember  EstimatedSalary  Exited  \n","0              1          1               1        101348.88       1  \n","1              1          0               1        112542.58       0  \n","2              3          1               0        113931.57       1  \n","3              2          0               0         93826.63       0  \n","4              1          1               1         79084.10       0  "]},"metadata":{"tags":[]},"execution_count":74}]},{"metadata":{"id":"3NxW7gdoex9i","colab_type":"text"},"cell_type":"markdown","source":["<h4>Encode object values</h4>"]},{"metadata":{"id":"2dhCiKXNdPbo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"27aa4674-506c-4fe7-a2ec-b896a9ba364a","executionInfo":{"status":"ok","timestamp":1549915065090,"user_tz":-330,"elapsed":23923,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["bank_df['Surname'].value_counts().unique()"],"execution_count":75,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([32, 29, 28, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13,\n","       12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1])"]},"metadata":{"tags":[]},"execution_count":75}]},{"metadata":{"id":"dqJoz83bWVG9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":316},"outputId":"e244e7a9-b11f-44a2-8d0d-944f8f647d80","executionInfo":{"status":"ok","timestamp":1549915065091,"user_tz":-330,"elapsed":23906,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["bank_df.info()"],"execution_count":76,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 10000 entries, 0 to 9999\n","Data columns (total 12 columns):\n","Surname            10000 non-null object\n","CreditScore        10000 non-null int64\n","Geography          10000 non-null object\n","Gender             10000 non-null object\n","Age                10000 non-null int64\n","Tenure             10000 non-null int64\n","Balance            10000 non-null float64\n","NumOfProducts      10000 non-null int64\n","HasCrCard          10000 non-null int64\n","IsActiveMember     10000 non-null int64\n","EstimatedSalary    10000 non-null float64\n","Exited             10000 non-null int64\n","dtypes: float64(2), int64(7), object(3)\n","memory usage: 937.6+ KB\n"],"name":"stdout"}]},{"metadata":{"id":"NmMzhqpQWXsp","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","le_surname = LabelEncoder()\n","le_geo = LabelEncoder()\n","le_gender = LabelEncoder()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ps-Er885Wxbi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"dff7c580-991e-405e-cc52-661a6d24e8fe","executionInfo":{"status":"ok","timestamp":1549915065105,"user_tz":-330,"elapsed":23894,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["bank_df['Surname'].value_counts().head()"],"execution_count":78,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Smith     32\n","Scott     29\n","Martin    29\n","Walker    28\n","Brown     26\n","Name: Surname, dtype: int64"]},"metadata":{"tags":[]},"execution_count":78}]},{"metadata":{"id":"PKAPUcCBW48y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"outputId":"747343d9-a944-4bf0-917d-61ed6a11c725","executionInfo":{"status":"ok","timestamp":1549915065106,"user_tz":-330,"elapsed":23876,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["bank_df['Geography'].value_counts()"],"execution_count":79,"outputs":[{"output_type":"execute_result","data":{"text/plain":["France     5014\n","Germany    2509\n","Spain      2477\n","Name: Geography, dtype: int64"]},"metadata":{"tags":[]},"execution_count":79}]},{"metadata":{"id":"o-SFGj8hW_Da","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"outputId":"ac0a5995-3cd7-45de-f5a7-384e62c9e59c","executionInfo":{"status":"ok","timestamp":1549915065107,"user_tz":-330,"elapsed":23854,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["bank_df['Gender'].value_counts()"],"execution_count":80,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Male      5457\n","Female    4543\n","Name: Gender, dtype: int64"]},"metadata":{"tags":[]},"execution_count":80}]},{"metadata":{"id":"7iloeCqnXBHi","colab_type":"code","colab":{}},"cell_type":"code","source":["bank_df['Surname1'] = le_surname.fit_transform(bank_df['Surname'])\n","bank_df['Geography1'] = le_geo.fit_transform(bank_df['Geography'])\n","bank_df['Gender1'] = le_geo.fit_transform(bank_df['Gender'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"51JyD4L_XOqD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"4a73c3a1-3a71-4481-eef0-2c53ed7caeb4","executionInfo":{"status":"ok","timestamp":1549915065110,"user_tz":-330,"elapsed":23824,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["bank_df['Surname1'].value_counts().head()"],"execution_count":82,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2473    32\n","1689    29\n","2389    29\n","2751    28\n","336     26\n","Name: Surname1, dtype: int64"]},"metadata":{"tags":[]},"execution_count":82}]},{"metadata":{"id":"II1KPK5-XQ4U","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"outputId":"49a2cb7f-9d5b-475f-fd71-05c4ecc29726","executionInfo":{"status":"ok","timestamp":1549915065110,"user_tz":-330,"elapsed":23803,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["bank_df['Geography1'].value_counts()"],"execution_count":83,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    5014\n","1    2509\n","2    2477\n","Name: Geography1, dtype: int64"]},"metadata":{"tags":[]},"execution_count":83}]},{"metadata":{"id":"8-X6ZLszXiby","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"outputId":"1200d933-6add-402a-dd12-6c01523247ca","executionInfo":{"status":"ok","timestamp":1549915065111,"user_tz":-330,"elapsed":23782,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["bank_df['Gender1'].value_counts()"],"execution_count":84,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    5457\n","0    4543\n","Name: Gender1, dtype: int64"]},"metadata":{"tags":[]},"execution_count":84}]},{"metadata":{"id":"JNu7xuDqXkm9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":369},"outputId":"f81c6115-cf0e-4944-c7e2-8f1d2ed797a7","executionInfo":{"status":"ok","timestamp":1549915065112,"user_tz":-330,"elapsed":23768,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["bank_df.info()"],"execution_count":85,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 10000 entries, 0 to 9999\n","Data columns (total 15 columns):\n","Surname            10000 non-null object\n","CreditScore        10000 non-null int64\n","Geography          10000 non-null object\n","Gender             10000 non-null object\n","Age                10000 non-null int64\n","Tenure             10000 non-null int64\n","Balance            10000 non-null float64\n","NumOfProducts      10000 non-null int64\n","HasCrCard          10000 non-null int64\n","IsActiveMember     10000 non-null int64\n","EstimatedSalary    10000 non-null float64\n","Exited             10000 non-null int64\n","Surname1           10000 non-null int64\n","Geography1         10000 non-null int64\n","Gender1            10000 non-null int64\n","dtypes: float64(2), int64(10), object(3)\n","memory usage: 1.1+ MB\n"],"name":"stdout"}]},{"metadata":{"id":"6c3ZceEKeOjq","colab_type":"text"},"cell_type":"markdown","source":["<h3>Dependant and Independant variables</h3>\n","\n","Defining X with all the independant variables including the newly formed columns with encoded values for the 3 columns - Geography, Gender and Surname. Surname may be considered as non-significant, but it may have some impact on this data. Hence including the same\n","\n","Defining Y with dependant variable - Exited"]},{"metadata":{"id":"X0y9h4jyXmru","colab_type":"code","colab":{}},"cell_type":"code","source":["X = bank_df[['CreditScore','Age','Tenure','Balance','NumOfProducts','HasCrCard','IsActiveMember','EstimatedSalary','Surname1','Geography1','Gender1']]\n","Y = bank_df[['Exited']]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OR-5uevrbnOf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"5fe8aa18-80b1-4ed0-be60-ec1965c87f76","executionInfo":{"status":"ok","timestamp":1549915065115,"user_tz":-330,"elapsed":23747,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["X.shape"],"execution_count":87,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10000, 11)"]},"metadata":{"tags":[]},"execution_count":87}]},{"metadata":{"id":"dgZIeb2uboEq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"0f37d9fb-2477-4594-d876-e6352acc4144","executionInfo":{"status":"ok","timestamp":1549915065115,"user_tz":-330,"elapsed":23731,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["Y.shape"],"execution_count":88,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10000, 1)"]},"metadata":{"tags":[]},"execution_count":88}]},{"metadata":{"id":"IwUwXeEEeKXw","colab_type":"text"},"cell_type":"markdown","source":["<h3>Train-Test split</h3>\n","\n","Train test split is done with 70:30 ratio and the shapes of each of the train test data for independant and dependant variables are printed below"]},{"metadata":{"id":"pn0f6XozboyP","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.3, random_state=7)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aRLWJc5Kb8AE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"aaf61de9-ab32-446b-8feb-c32fdaaa7a1a","executionInfo":{"status":"ok","timestamp":1549915065121,"user_tz":-330,"elapsed":23714,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["X_train.shape"],"execution_count":90,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(7000, 11)"]},"metadata":{"tags":[]},"execution_count":90}]},{"metadata":{"id":"uU5u2xzPcI0i","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"8a1b4721-14da-4ee4-de70-fc1da109caf3","executionInfo":{"status":"ok","timestamp":1549915065122,"user_tz":-330,"elapsed":23702,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["X_test.shape"],"execution_count":91,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3000, 11)"]},"metadata":{"tags":[]},"execution_count":91}]},{"metadata":{"id":"1nx0q8bjcKBQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"08b2b845-7f15-48f4-f669-58c4a52de92a","executionInfo":{"status":"ok","timestamp":1549915065445,"user_tz":-330,"elapsed":24008,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["Y_train.shape"],"execution_count":92,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(7000, 1)"]},"metadata":{"tags":[]},"execution_count":92}]},{"metadata":{"id":"4C9cpXNhcLJE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"75566765-b862-4131-90c5-244cd2ccb384","executionInfo":{"status":"ok","timestamp":1549915065447,"user_tz":-330,"elapsed":23993,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["Y_test.shape"],"execution_count":93,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3000, 1)"]},"metadata":{"tags":[]},"execution_count":93}]},{"metadata":{"id":"dTJ5n38oeEts","colab_type":"text"},"cell_type":"markdown","source":["<h3>Normalization of data</h3>\n","\n","All the variables are in different scales and hence, normalization is critical. Care is taken to normalize the data after train-test split to reduce the effect of test data in order to avoid bias errors"]},{"metadata":{"id":"_0t1SYoRcMCF","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","ss = StandardScaler()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"V32sLlNVcZKP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":178},"outputId":"e6d483a6-f6c9-47e3-aae8-ab31c85a7e27","executionInfo":{"status":"ok","timestamp":1549915065452,"user_tz":-330,"elapsed":23959,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["X_train_ss = ss.fit_transform(X_train)\n","X_test_ss = ss.fit_transform(X_test)\n","#Y_train_ss = ss.fit_transform(Y_train)\n","#Y_test_ss = ss.fit_transform(Y_test)"],"execution_count":95,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n","  return self.partial_fit(X, y)\n","/usr/local/lib/python3.6/dist-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n","  return self.fit(X, **fit_params).transform(X)\n","/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n","  return self.partial_fit(X, y)\n","/usr/local/lib/python3.6/dist-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n","  return self.fit(X, **fit_params).transform(X)\n"],"name":"stderr"}]},{"metadata":{"id":"lGBxjOeAcuUJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":298},"outputId":"57930379-538a-4c91-ead0-e5326238a3f7","executionInfo":{"status":"ok","timestamp":1549915065453,"user_tz":-330,"elapsed":23944,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["X_train.info()"],"execution_count":96,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 7000 entries, 2317 to 9412\n","Data columns (total 11 columns):\n","CreditScore        7000 non-null int64\n","Age                7000 non-null int64\n","Tenure             7000 non-null int64\n","Balance            7000 non-null float64\n","NumOfProducts      7000 non-null int64\n","HasCrCard          7000 non-null int64\n","IsActiveMember     7000 non-null int64\n","EstimatedSalary    7000 non-null float64\n","Surname1           7000 non-null int64\n","Geography1         7000 non-null int64\n","Gender1            7000 non-null int64\n","dtypes: float64(2), int64(9)\n","memory usage: 656.2 KB\n"],"name":"stdout"}]},{"metadata":{"id":"Kymfv-dycyku","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":298},"outputId":"08409071-8082-430f-94d2-155ea9a6f512","executionInfo":{"status":"ok","timestamp":1549915065454,"user_tz":-330,"elapsed":23917,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["X_test.info()"],"execution_count":97,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 3000 entries, 1977 to 3582\n","Data columns (total 11 columns):\n","CreditScore        3000 non-null int64\n","Age                3000 non-null int64\n","Tenure             3000 non-null int64\n","Balance            3000 non-null float64\n","NumOfProducts      3000 non-null int64\n","HasCrCard          3000 non-null int64\n","IsActiveMember     3000 non-null int64\n","EstimatedSalary    3000 non-null float64\n","Surname1           3000 non-null int64\n","Geography1         3000 non-null int64\n","Gender1            3000 non-null int64\n","dtypes: float64(2), int64(9)\n","memory usage: 281.2 KB\n"],"name":"stdout"}]},{"metadata":{"id":"EGPeRQVfc0MR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"8807632f-b4a1-414b-dac9-1d99c074ce69","executionInfo":{"status":"ok","timestamp":1549915065455,"user_tz":-330,"elapsed":23898,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["Y_train.info()"],"execution_count":98,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 7000 entries, 2317 to 9412\n","Data columns (total 1 columns):\n","Exited    7000 non-null int64\n","dtypes: int64(1)\n","memory usage: 109.4 KB\n"],"name":"stdout"}]},{"metadata":{"id":"LqJyMEtGc1xA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"ccb01a9d-4e0e-40a2-f908-f979232cf31e","executionInfo":{"status":"ok","timestamp":1549915065455,"user_tz":-330,"elapsed":23877,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["Y_test.info()"],"execution_count":99,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 3000 entries, 1977 to 3582\n","Data columns (total 1 columns):\n","Exited    3000 non-null int64\n","dtypes: int64(1)\n","memory usage: 46.9 KB\n"],"name":"stdout"}]},{"metadata":{"id":"8Yn2Y4UceAED","colab_type":"text"},"cell_type":"markdown","source":["<h3>Logistic Regression</h3>\n","\n","Logistic regression is performed here with scikit learn to find out the accuracy/confusion matrix/precision & recall score/f1-score of the model. Later this can be used to compare with the accuracy obtained using neural network\n","\n","\n","*   78.93% accuracy with logistic regression\n","*   F1-score of 88% for not exited use case and 8% for exited use case is obtained as shown below\n","\n"]},{"metadata":{"id":"kwEefQPWc204","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":318},"outputId":"5170b1dd-d91b-4c6b-a308-9cbc4cf6a026","executionInfo":{"status":"ok","timestamp":1549915065457,"user_tz":-330,"elapsed":23861,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","lr = LogisticRegression()\n","lr.fit(X_train_ss,Y_train)\n","print(lr.score(X_test_ss,Y_test))\n","Y_pred = lr.predict(X_test_ss)\n","from sklearn.metrics import classification_report, confusion_matrix\n","print(classification_report(Y_test,Y_pred))\n","print(confusion_matrix(Y_test,Y_pred))\n"],"execution_count":100,"outputs":[{"output_type":"stream","text":["0.8116666666666666\n","              precision    recall  f1-score   support\n","\n","           0       0.82      0.97      0.89      2395\n","           1       0.61      0.19      0.28       605\n","\n","   micro avg       0.81      0.81      0.81      3000\n","   macro avg       0.72      0.58      0.59      3000\n","weighted avg       0.78      0.81      0.77      3000\n","\n","[[2323   72]\n"," [ 493  112]]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"],"name":"stderr"}]},{"metadata":{"id":"1EkC0_qvCNSO","colab_type":"text"},"cell_type":"markdown","source":["<h3>Keras</h3>"]},{"metadata":{"id":"6rm8mznoynu7","colab_type":"text"},"cell_type":"markdown","source":["<h4>Neural Network with no hidden layer</h4>\n","\n","Neural network with no hidden layer has only input layer with 11 variables and 1 output/dependant variables. Default learning rate with batch value of 11 along with epochs of 50 is used\n","\n","Sigmoid function gives values from 0 to 1 for the test set. With 0.5 as threshold, if the predicted value is > 0.5, output will be 1 and if the predicted value < 0.5, output will be 0\n","\n","This Neural Network gives an accuracy of 81.3% and f1-score of 89% (true) and 30% (false) and 77% as weighted average"]},{"metadata":{"id":"zUB1Iy-pCpwG","colab_type":"code","colab":{}},"cell_type":"code","source":["import keras\n","import tensorflow as tf\n","from sklearn.metrics import accuracy_score"],"execution_count":0,"outputs":[]},{"metadata":{"id":"X548C3Oh8BMG","colab_type":"text"},"cell_type":"markdown","source":["<h5>Convert Y_pred to binary</h5>\n","\n","Convert Y_pred to binary values based on 0.5 as threshold"]},{"metadata":{"id":"rdvU3AKS73hE","colab_type":"code","colab":{}},"cell_type":"code","source":["def to_y_pred_bin(Y_pred_input):\n","  Y_pred_output = Y_pred_input\n","  for i in range(Y_pred_input.shape[0]):\n","\n","          # Convert probabilities A[0,i] to actual predictions p[0,i]\n","          ### START CODE HERE ### (â‰ˆ 4 lines of code)\n","          if Y_pred_input[i ,0] >= 0.5:\n","              Y_pred_output[i ,0] = 1\n","          else:\n","              Y_pred_output[i,0] = 0\n","          ### END CODE HERE ###\n","  return Y_pred_output"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5jpH-s66r7ye","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":2006},"outputId":"a3d38e84-2ae0-4baf-fe7d-a3611f4aca21","executionInfo":{"status":"ok","timestamp":1549915116931,"user_tz":-330,"elapsed":75305,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["#Initialize Sequential model\n","model = tf.keras.models.Sequential()\n","\n","#Reshape data from 2D to 1D\n","model.add(tf.keras.layers.Reshape((11,),input_shape=(11,)))\n","\n","#Normalize the data\n","model.add(tf.keras.layers.BatchNormalization())\n","\n","#Add Dense Layer which provides 1 Output after applying sigmoid\n","model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n","\n","\n","#Compile the model\n","model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","model.fit(X_train_ss, Y_train, \n","          validation_data=(X_test_ss, Y_test), \n","          epochs=50,\n","          batch_size=X_train_ss.shape[1])\n","\n","Y_NoHidden_pred = model.predict(X_test_ss)\n","Z_NoHidden_pred = to_y_pred_bin(Y_NoHidden_pred)\n","\n","print(accuracy_score(Y_test,Z_NoHidden_pred))\n","print(confusion_matrix(Y_test,Z_NoHidden_pred))\n","print(classification_report(Y_test,Z_NoHidden_pred))"],"execution_count":103,"outputs":[{"output_type":"stream","text":["Train on 7000 samples, validate on 3000 samples\n","Epoch 1/50\n","7000/7000 [==============================] - 2s 215us/sample - loss: 0.5673 - acc: 0.7204 - val_loss: 0.4868 - val_acc: 0.7750\n","Epoch 2/50\n","7000/7000 [==============================] - 1s 144us/sample - loss: 0.4627 - acc: 0.7900 - val_loss: 0.4516 - val_acc: 0.7937\n","Epoch 3/50\n","7000/7000 [==============================] - 1s 139us/sample - loss: 0.4480 - acc: 0.7981 - val_loss: 0.4397 - val_acc: 0.8043\n","Epoch 4/50\n","7000/7000 [==============================] - 1s 136us/sample - loss: 0.4399 - acc: 0.8056 - val_loss: 0.4370 - val_acc: 0.8067\n","Epoch 5/50\n","7000/7000 [==============================] - 1s 139us/sample - loss: 0.4402 - acc: 0.8083 - val_loss: 0.4347 - val_acc: 0.8107\n","Epoch 6/50\n","7000/7000 [==============================] - 1s 142us/sample - loss: 0.4354 - acc: 0.8101 - val_loss: 0.4350 - val_acc: 0.8050\n","Epoch 7/50\n","7000/7000 [==============================] - 1s 137us/sample - loss: 0.4373 - acc: 0.8069 - val_loss: 0.4343 - val_acc: 0.8073\n","Epoch 8/50\n","7000/7000 [==============================] - 1s 136us/sample - loss: 0.4381 - acc: 0.8103 - val_loss: 0.4352 - val_acc: 0.8053\n","Epoch 9/50\n","7000/7000 [==============================] - 1s 136us/sample - loss: 0.4378 - acc: 0.8070 - val_loss: 0.4336 - val_acc: 0.8100\n","Epoch 10/50\n","7000/7000 [==============================] - 1s 139us/sample - loss: 0.4373 - acc: 0.8096 - val_loss: 0.4333 - val_acc: 0.8107\n","Epoch 11/50\n","7000/7000 [==============================] - 1s 135us/sample - loss: 0.4363 - acc: 0.8126 - val_loss: 0.4344 - val_acc: 0.8067\n","Epoch 12/50\n","7000/7000 [==============================] - 1s 136us/sample - loss: 0.4358 - acc: 0.8041 - val_loss: 0.4337 - val_acc: 0.8083\n","Epoch 13/50\n","7000/7000 [==============================] - 1s 134us/sample - loss: 0.4381 - acc: 0.8103 - val_loss: 0.4332 - val_acc: 0.8103\n","Epoch 14/50\n","7000/7000 [==============================] - 1s 137us/sample - loss: 0.4386 - acc: 0.8076 - val_loss: 0.4334 - val_acc: 0.8110\n","Epoch 15/50\n","7000/7000 [==============================] - 1s 138us/sample - loss: 0.4342 - acc: 0.8104 - val_loss: 0.4334 - val_acc: 0.8083\n","Epoch 16/50\n","7000/7000 [==============================] - 1s 139us/sample - loss: 0.4363 - acc: 0.8099 - val_loss: 0.4336 - val_acc: 0.8093\n","Epoch 17/50\n","7000/7000 [==============================] - 1s 137us/sample - loss: 0.4359 - acc: 0.8087 - val_loss: 0.4328 - val_acc: 0.8123\n","Epoch 18/50\n","7000/7000 [==============================] - 1s 139us/sample - loss: 0.4353 - acc: 0.8111 - val_loss: 0.4331 - val_acc: 0.8117\n","Epoch 19/50\n","7000/7000 [==============================] - 1s 136us/sample - loss: 0.4389 - acc: 0.8087 - val_loss: 0.4331 - val_acc: 0.8090\n","Epoch 20/50\n","7000/7000 [==============================] - 1s 139us/sample - loss: 0.4371 - acc: 0.8083 - val_loss: 0.4331 - val_acc: 0.8103\n","Epoch 21/50\n","7000/7000 [==============================] - 1s 134us/sample - loss: 0.4402 - acc: 0.8101 - val_loss: 0.4324 - val_acc: 0.8110\n","Epoch 22/50\n","7000/7000 [==============================] - 1s 137us/sample - loss: 0.4355 - acc: 0.8109 - val_loss: 0.4326 - val_acc: 0.8113\n","Epoch 23/50\n","7000/7000 [==============================] - 1s 138us/sample - loss: 0.4362 - acc: 0.8101 - val_loss: 0.4329 - val_acc: 0.8110\n","Epoch 24/50\n","7000/7000 [==============================] - 1s 135us/sample - loss: 0.4343 - acc: 0.8110 - val_loss: 0.4345 - val_acc: 0.8057\n","Epoch 25/50\n","7000/7000 [==============================] - 1s 134us/sample - loss: 0.4401 - acc: 0.8064 - val_loss: 0.4326 - val_acc: 0.8127\n","Epoch 26/50\n","7000/7000 [==============================] - 1s 137us/sample - loss: 0.4368 - acc: 0.8124 - val_loss: 0.4332 - val_acc: 0.8107\n","Epoch 27/50\n","7000/7000 [==============================] - 1s 138us/sample - loss: 0.4339 - acc: 0.8131 - val_loss: 0.4337 - val_acc: 0.8080\n","Epoch 28/50\n","7000/7000 [==============================] - 1s 137us/sample - loss: 0.4361 - acc: 0.8121 - val_loss: 0.4337 - val_acc: 0.8077\n","Epoch 29/50\n","7000/7000 [==============================] - 1s 137us/sample - loss: 0.4370 - acc: 0.8061 - val_loss: 0.4334 - val_acc: 0.8083\n","Epoch 30/50\n","7000/7000 [==============================] - 1s 138us/sample - loss: 0.4342 - acc: 0.8097 - val_loss: 0.4334 - val_acc: 0.8087\n","Epoch 31/50\n","7000/7000 [==============================] - 1s 137us/sample - loss: 0.4374 - acc: 0.8084 - val_loss: 0.4333 - val_acc: 0.8093\n","Epoch 32/50\n","7000/7000 [==============================] - 1s 140us/sample - loss: 0.4403 - acc: 0.8077 - val_loss: 0.4331 - val_acc: 0.8110\n","Epoch 33/50\n","7000/7000 [==============================] - 1s 139us/sample - loss: 0.4360 - acc: 0.8113 - val_loss: 0.4329 - val_acc: 0.8110\n","Epoch 34/50\n","7000/7000 [==============================] - 1s 135us/sample - loss: 0.4375 - acc: 0.8086 - val_loss: 0.4331 - val_acc: 0.8097\n","Epoch 35/50\n","7000/7000 [==============================] - 1s 137us/sample - loss: 0.4333 - acc: 0.8123 - val_loss: 0.4330 - val_acc: 0.8127\n","Epoch 36/50\n","7000/7000 [==============================] - 1s 136us/sample - loss: 0.4379 - acc: 0.8064 - val_loss: 0.4339 - val_acc: 0.8083\n","Epoch 37/50\n","7000/7000 [==============================] - 1s 135us/sample - loss: 0.4387 - acc: 0.8133 - val_loss: 0.4332 - val_acc: 0.8107\n","Epoch 38/50\n","7000/7000 [==============================] - 1s 137us/sample - loss: 0.4385 - acc: 0.8137 - val_loss: 0.4335 - val_acc: 0.8083\n","Epoch 39/50\n","7000/7000 [==============================] - 1s 134us/sample - loss: 0.4346 - acc: 0.8101 - val_loss: 0.4334 - val_acc: 0.8097\n","Epoch 40/50\n","7000/7000 [==============================] - 1s 134us/sample - loss: 0.4378 - acc: 0.8106 - val_loss: 0.4332 - val_acc: 0.8117\n","Epoch 41/50\n","7000/7000 [==============================] - 1s 135us/sample - loss: 0.4357 - acc: 0.8117 - val_loss: 0.4330 - val_acc: 0.8113\n","Epoch 42/50\n","7000/7000 [==============================] - 1s 136us/sample - loss: 0.4340 - acc: 0.8110 - val_loss: 0.4335 - val_acc: 0.8097\n","Epoch 43/50\n","7000/7000 [==============================] - 1s 137us/sample - loss: 0.4385 - acc: 0.8101 - val_loss: 0.4342 - val_acc: 0.8060\n","Epoch 44/50\n","7000/7000 [==============================] - 1s 138us/sample - loss: 0.4390 - acc: 0.8067 - val_loss: 0.4331 - val_acc: 0.8093\n","Epoch 45/50\n","7000/7000 [==============================] - 1s 138us/sample - loss: 0.4371 - acc: 0.8094 - val_loss: 0.4335 - val_acc: 0.8100\n","Epoch 46/50\n","7000/7000 [==============================] - 1s 134us/sample - loss: 0.4364 - acc: 0.8110 - val_loss: 0.4333 - val_acc: 0.8100\n","Epoch 47/50\n","7000/7000 [==============================] - 1s 134us/sample - loss: 0.4355 - acc: 0.8119 - val_loss: 0.4336 - val_acc: 0.8077\n","Epoch 48/50\n","7000/7000 [==============================] - 1s 136us/sample - loss: 0.4386 - acc: 0.8101 - val_loss: 0.4333 - val_acc: 0.8110\n","Epoch 49/50\n","7000/7000 [==============================] - 1s 138us/sample - loss: 0.4374 - acc: 0.8094 - val_loss: 0.4330 - val_acc: 0.8117\n","Epoch 50/50\n","7000/7000 [==============================] - 1s 136us/sample - loss: 0.4402 - acc: 0.8051 - val_loss: 0.4327 - val_acc: 0.8113\n","0.8113333333333334\n","[[2314   81]\n"," [ 485  120]]\n","              precision    recall  f1-score   support\n","\n","           0       0.83      0.97      0.89      2395\n","           1       0.60      0.20      0.30       605\n","\n","   micro avg       0.81      0.81      0.81      3000\n","   macro avg       0.71      0.58      0.59      3000\n","weighted avg       0.78      0.81      0.77      3000\n","\n"],"name":"stdout"}]},{"metadata":{"id":"CgHL--YBI8jZ","colab_type":"text"},"cell_type":"markdown","source":["<h4>Neural Network with no hidden layer with learning rate of 0.03</h4>\n","\n","Neural network with no hidden layer has only input layer with 11 variables and 1 output/dependant variables. Learning rate of 0.03/0.1 along with batch value of 11 with epochs as 100 is used\n","\n","Sigmoid function gives values from 0 to 1 for the test set. With 0.5 as threshold, if the predicted value is > 0.5, output will be 1 and if the predicted value < 0.5, output will be 0\n","\n","This Neural Network gives an accuracy of 81.3% and f1-score of 89% (true) and 30% (false) and 77% as weighted average"]},{"metadata":{"id":"2a3U_PE8Dl_-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":2006},"outputId":"16ab0e8f-92ba-41e3-a0cb-9a099ba2f203","executionInfo":{"status":"ok","timestamp":1549915169038,"user_tz":-330,"elapsed":127396,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["#Initialize Sequential model\n","model = tf.keras.models.Sequential()\n","\n","#Reshape data from 2D to 1D\n","model.add(tf.keras.layers.Reshape((11,),input_shape=(11,)))\n","\n","#Normalize the data\n","model.add(tf.keras.layers.BatchNormalization())\n","\n","#Add Dense Layer which provides 1 Output after applying softmax\n","model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n","\n","\n","#Comile the model\n","sgd_optimizer = tf.keras.optimizers.SGD(lr=0.1)\n","model.compile(optimizer=sgd_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","\n","model.fit(X_train_ss, Y_train, \n","          validation_data=(X_test_ss, Y_test), \n","          epochs=50,\n","          batch_size=X_train_ss.shape[1])\n","\n","Y_lr_pred = model.predict(X_test_ss)\n","Z_lr_pred = to_y_pred_bin(Y_lr_pred)\n","\n","print(accuracy_score(Y_test,Z_lr_pred))\n","print(confusion_matrix(Y_test,Z_lr_pred))\n","print(classification_report(Y_test,Z_lr_pred))"],"execution_count":104,"outputs":[{"output_type":"stream","text":["Train on 7000 samples, validate on 3000 samples\n","Epoch 1/50\n","7000/7000 [==============================] - 2s 217us/sample - loss: 0.4713 - acc: 0.7896 - val_loss: 0.4363 - val_acc: 0.8080\n","Epoch 2/50\n","7000/7000 [==============================] - 1s 138us/sample - loss: 0.4437 - acc: 0.8069 - val_loss: 0.4351 - val_acc: 0.8063\n","Epoch 3/50\n","7000/7000 [==============================] - 1s 137us/sample - loss: 0.4422 - acc: 0.8057 - val_loss: 0.4355 - val_acc: 0.8113\n","Epoch 4/50\n","7000/7000 [==============================] - 1s 138us/sample - loss: 0.4403 - acc: 0.8073 - val_loss: 0.4322 - val_acc: 0.8117\n","Epoch 5/50\n","7000/7000 [==============================] - 1s 135us/sample - loss: 0.4390 - acc: 0.8106 - val_loss: 0.4384 - val_acc: 0.8020\n","Epoch 6/50\n","7000/7000 [==============================] - 1s 137us/sample - loss: 0.4408 - acc: 0.8056 - val_loss: 0.4337 - val_acc: 0.8103\n","Epoch 7/50\n","7000/7000 [==============================] - 1s 136us/sample - loss: 0.4385 - acc: 0.8073 - val_loss: 0.4342 - val_acc: 0.8073\n","Epoch 8/50\n","7000/7000 [==============================] - 1s 139us/sample - loss: 0.4399 - acc: 0.8104 - val_loss: 0.4368 - val_acc: 0.8117\n","Epoch 9/50\n","7000/7000 [==============================] - 1s 138us/sample - loss: 0.4368 - acc: 0.8113 - val_loss: 0.4392 - val_acc: 0.8020\n","Epoch 10/50\n","7000/7000 [==============================] - 1s 140us/sample - loss: 0.4383 - acc: 0.8089 - val_loss: 0.4370 - val_acc: 0.8063\n","Epoch 11/50\n","7000/7000 [==============================] - 1s 139us/sample - loss: 0.4378 - acc: 0.8087 - val_loss: 0.4382 - val_acc: 0.8040\n","Epoch 12/50\n","7000/7000 [==============================] - 1s 137us/sample - loss: 0.4398 - acc: 0.8089 - val_loss: 0.4360 - val_acc: 0.8120\n","Epoch 13/50\n","7000/7000 [==============================] - 1s 138us/sample - loss: 0.4399 - acc: 0.8087 - val_loss: 0.4362 - val_acc: 0.8073\n","Epoch 14/50\n","7000/7000 [==============================] - 1s 139us/sample - loss: 0.4427 - acc: 0.8029 - val_loss: 0.4365 - val_acc: 0.8157\n","Epoch 15/50\n","7000/7000 [==============================] - 1s 138us/sample - loss: 0.4412 - acc: 0.8074 - val_loss: 0.4348 - val_acc: 0.8103\n","Epoch 16/50\n","7000/7000 [==============================] - 1s 139us/sample - loss: 0.4399 - acc: 0.8079 - val_loss: 0.4332 - val_acc: 0.8090\n","Epoch 17/50\n","7000/7000 [==============================] - 1s 139us/sample - loss: 0.4391 - acc: 0.8103 - val_loss: 0.4332 - val_acc: 0.8137\n","Epoch 18/50\n","7000/7000 [==============================] - 1s 139us/sample - loss: 0.4420 - acc: 0.8050 - val_loss: 0.4338 - val_acc: 0.8120\n","Epoch 19/50\n","7000/7000 [==============================] - 1s 135us/sample - loss: 0.4370 - acc: 0.8093 - val_loss: 0.4340 - val_acc: 0.8080\n","Epoch 20/50\n","7000/7000 [==============================] - 1s 135us/sample - loss: 0.4374 - acc: 0.8111 - val_loss: 0.4346 - val_acc: 0.8047\n","Epoch 21/50\n","7000/7000 [==============================] - 1s 138us/sample - loss: 0.4393 - acc: 0.8087 - val_loss: 0.4426 - val_acc: 0.8063\n","Epoch 22/50\n","7000/7000 [==============================] - 1s 135us/sample - loss: 0.4396 - acc: 0.8096 - val_loss: 0.4320 - val_acc: 0.8103\n","Epoch 23/50\n","7000/7000 [==============================] - 1s 137us/sample - loss: 0.4380 - acc: 0.8087 - val_loss: 0.4352 - val_acc: 0.8040\n","Epoch 24/50\n","7000/7000 [==============================] - 1s 138us/sample - loss: 0.4390 - acc: 0.8087 - val_loss: 0.4372 - val_acc: 0.8037\n","Epoch 25/50\n","7000/7000 [==============================] - 1s 138us/sample - loss: 0.4390 - acc: 0.8081 - val_loss: 0.4336 - val_acc: 0.8077\n","Epoch 26/50\n","7000/7000 [==============================] - 1s 138us/sample - loss: 0.4408 - acc: 0.8059 - val_loss: 0.4356 - val_acc: 0.8190\n","Epoch 27/50\n","7000/7000 [==============================] - 1s 137us/sample - loss: 0.4402 - acc: 0.8081 - val_loss: 0.4360 - val_acc: 0.8087\n","Epoch 28/50\n","7000/7000 [==============================] - 1s 140us/sample - loss: 0.4397 - acc: 0.8083 - val_loss: 0.4345 - val_acc: 0.8053\n","Epoch 29/50\n","7000/7000 [==============================] - 1s 137us/sample - loss: 0.4396 - acc: 0.8071 - val_loss: 0.4328 - val_acc: 0.8123\n","Epoch 30/50\n","7000/7000 [==============================] - 1s 139us/sample - loss: 0.4395 - acc: 0.8087 - val_loss: 0.4342 - val_acc: 0.8057\n","Epoch 31/50\n","7000/7000 [==============================] - 1s 138us/sample - loss: 0.4414 - acc: 0.8071 - val_loss: 0.4349 - val_acc: 0.8030\n","Epoch 32/50\n","7000/7000 [==============================] - 1s 139us/sample - loss: 0.4390 - acc: 0.8049 - val_loss: 0.4375 - val_acc: 0.8070\n","Epoch 33/50\n","7000/7000 [==============================] - 1s 138us/sample - loss: 0.4395 - acc: 0.8099 - val_loss: 0.4377 - val_acc: 0.8087\n","Epoch 34/50\n","7000/7000 [==============================] - 1s 137us/sample - loss: 0.4399 - acc: 0.8076 - val_loss: 0.4333 - val_acc: 0.8100\n","Epoch 35/50\n","7000/7000 [==============================] - 1s 139us/sample - loss: 0.4381 - acc: 0.8093 - val_loss: 0.4377 - val_acc: 0.8030\n","Epoch 36/50\n","7000/7000 [==============================] - 1s 137us/sample - loss: 0.4387 - acc: 0.8117 - val_loss: 0.4336 - val_acc: 0.8123\n","Epoch 37/50\n","7000/7000 [==============================] - 1s 138us/sample - loss: 0.4376 - acc: 0.8117 - val_loss: 0.4345 - val_acc: 0.8073\n","Epoch 38/50\n","7000/7000 [==============================] - 1s 138us/sample - loss: 0.4400 - acc: 0.8067 - val_loss: 0.4362 - val_acc: 0.8060\n","Epoch 39/50\n","7000/7000 [==============================] - 1s 138us/sample - loss: 0.4386 - acc: 0.8061 - val_loss: 0.4342 - val_acc: 0.8063\n","Epoch 40/50\n","7000/7000 [==============================] - 1s 137us/sample - loss: 0.4367 - acc: 0.8079 - val_loss: 0.4349 - val_acc: 0.8077\n","Epoch 41/50\n","7000/7000 [==============================] - 1s 141us/sample - loss: 0.4388 - acc: 0.8083 - val_loss: 0.4344 - val_acc: 0.8113\n","Epoch 42/50\n","7000/7000 [==============================] - 1s 139us/sample - loss: 0.4378 - acc: 0.8130 - val_loss: 0.4350 - val_acc: 0.8063\n","Epoch 43/50\n","7000/7000 [==============================] - 1s 137us/sample - loss: 0.4396 - acc: 0.8071 - val_loss: 0.4323 - val_acc: 0.8123\n","Epoch 44/50\n","7000/7000 [==============================] - 1s 137us/sample - loss: 0.4412 - acc: 0.8081 - val_loss: 0.4323 - val_acc: 0.8130\n","Epoch 45/50\n","7000/7000 [==============================] - 1s 139us/sample - loss: 0.4373 - acc: 0.8086 - val_loss: 0.4458 - val_acc: 0.7993\n","Epoch 46/50\n","7000/7000 [==============================] - 1s 135us/sample - loss: 0.4360 - acc: 0.8094 - val_loss: 0.4352 - val_acc: 0.8153\n","Epoch 47/50\n","7000/7000 [==============================] - 1s 138us/sample - loss: 0.4406 - acc: 0.8097 - val_loss: 0.4343 - val_acc: 0.8123\n","Epoch 48/50\n","7000/7000 [==============================] - 1s 136us/sample - loss: 0.4390 - acc: 0.8110 - val_loss: 0.4348 - val_acc: 0.8087\n","Epoch 49/50\n","7000/7000 [==============================] - 1s 138us/sample - loss: 0.4386 - acc: 0.8091 - val_loss: 0.4351 - val_acc: 0.8127\n","Epoch 50/50\n","7000/7000 [==============================] - 1s 137us/sample - loss: 0.4360 - acc: 0.8131 - val_loss: 0.4364 - val_acc: 0.8080\n","0.808\n","[[2364   31]\n"," [ 545   60]]\n","              precision    recall  f1-score   support\n","\n","           0       0.81      0.99      0.89      2395\n","           1       0.66      0.10      0.17       605\n","\n","   micro avg       0.81      0.81      0.81      3000\n","   macro avg       0.74      0.54      0.53      3000\n","weighted avg       0.78      0.81      0.75      3000\n","\n"],"name":"stdout"}]},{"metadata":{"id":"wMeurpw1Vayt","colab_type":"text"},"cell_type":"markdown","source":["<h4>Optimization</h4>\n","<h4>Neural Network with  no hidden layer (batch value=52)</h4>\n","\n","Neural network with no hidden layer has only input layer with 11 variables and 1 output/dependant variables. Default learning rate is used and batch value of 52 is used\n","\n","Sigmoid function gives values from 0 to 1 for the test set. With 0.5 as threshold, if the predicted value is > 0.5, output will be 1 and if the predicted value < 0.5, output will be 0\n","\n","This Neural Network gives an accuracy of 81.3% and f1-score of 89% (true) and 30% (false) and 77% as weighted average"]},{"metadata":{"id":"87yxkrSIVZww","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":3766},"outputId":"a366df46-6f54-467a-c95e-9cc1e17b9ff1","executionInfo":{"status":"ok","timestamp":1549915199410,"user_tz":-330,"elapsed":157752,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["#Initialize Sequential model\n","model = tf.keras.models.Sequential()\n","\n","#Reshape data from 2D to 1D\n","model.add(tf.keras.layers.Reshape((11,),input_shape=(11,)))\n","\n","#Normalize the data\n","model.add(tf.keras.layers.BatchNormalization())\n","\n","#Add Dense Layer which provides 1 Output after applying softmax\n","model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n","\n","\n","#Comile the model\n","sgd_optimizer = tf.keras.optimizers.SGD(lr=0.03)\n","model.compile(optimizer=\"sgd\", loss='binary_crossentropy', metrics=['accuracy'])\n","\n","model.fit(X_train_ss,Y_train, validation_data=(X_test_ss,Y_test),\n","          epochs=100,batch_size=52)\n","\n","Y_pred_batch = model.predict(X_test_ss)\n","Z_pred_batch = to_y_pred_bin(Y_pred_batch)\n","\n","print(accuracy_score(Y_test,Y_pred_batch))\n","print(confusion_matrix(Y_test,Y_pred_batch))\n","print(classification_report(Y_test,Y_pred_batch))"],"execution_count":105,"outputs":[{"output_type":"stream","text":["Train on 7000 samples, validate on 3000 samples\n","Epoch 1/100\n","7000/7000 [==============================] - 1s 116us/sample - loss: 0.7347 - acc: 0.6084 - val_loss: 0.6333 - val_acc: 0.6850\n","Epoch 2/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.5873 - acc: 0.7123 - val_loss: 0.5543 - val_acc: 0.7393\n","Epoch 3/100\n","7000/7000 [==============================] - 0s 33us/sample - loss: 0.5357 - acc: 0.7487 - val_loss: 0.5181 - val_acc: 0.7607\n","Epoch 4/100\n","7000/7000 [==============================] - 0s 33us/sample - loss: 0.5082 - acc: 0.7604 - val_loss: 0.4979 - val_acc: 0.7700\n","Epoch 5/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4913 - acc: 0.7720 - val_loss: 0.4843 - val_acc: 0.7777\n","Epoch 6/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4810 - acc: 0.7777 - val_loss: 0.4747 - val_acc: 0.7797\n","Epoch 7/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4728 - acc: 0.7827 - val_loss: 0.4673 - val_acc: 0.7850\n","Epoch 8/100\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4672 - acc: 0.7806 - val_loss: 0.4617 - val_acc: 0.7880\n","Epoch 9/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4630 - acc: 0.7826 - val_loss: 0.4569 - val_acc: 0.7903\n","Epoch 10/100\n","7000/7000 [==============================] - 0s 33us/sample - loss: 0.4579 - acc: 0.7844 - val_loss: 0.4530 - val_acc: 0.7917\n","Epoch 11/100\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4553 - acc: 0.7866 - val_loss: 0.4498 - val_acc: 0.7923\n","Epoch 12/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4518 - acc: 0.7881 - val_loss: 0.4471 - val_acc: 0.7923\n","Epoch 13/100\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4508 - acc: 0.7904 - val_loss: 0.4447 - val_acc: 0.7940\n","Epoch 14/100\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4453 - acc: 0.7943 - val_loss: 0.4426 - val_acc: 0.7960\n","Epoch 15/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4466 - acc: 0.7916 - val_loss: 0.4408 - val_acc: 0.7987\n","Epoch 16/100\n","7000/7000 [==============================] - 0s 33us/sample - loss: 0.4426 - acc: 0.7951 - val_loss: 0.4393 - val_acc: 0.8000\n","Epoch 17/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4409 - acc: 0.7956 - val_loss: 0.4379 - val_acc: 0.8023\n","Epoch 18/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4404 - acc: 0.7989 - val_loss: 0.4368 - val_acc: 0.8040\n","Epoch 19/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4397 - acc: 0.7993 - val_loss: 0.4359 - val_acc: 0.8043\n","Epoch 20/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4412 - acc: 0.8009 - val_loss: 0.4351 - val_acc: 0.8050\n","Epoch 21/100\n","7000/7000 [==============================] - 0s 33us/sample - loss: 0.4392 - acc: 0.8016 - val_loss: 0.4344 - val_acc: 0.8060\n","Epoch 22/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4385 - acc: 0.8020 - val_loss: 0.4340 - val_acc: 0.8060\n","Epoch 23/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4376 - acc: 0.8039 - val_loss: 0.4336 - val_acc: 0.8060\n","Epoch 24/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4391 - acc: 0.7993 - val_loss: 0.4332 - val_acc: 0.8073\n","Epoch 25/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4365 - acc: 0.8044 - val_loss: 0.4329 - val_acc: 0.8080\n","Epoch 26/100\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4362 - acc: 0.8036 - val_loss: 0.4327 - val_acc: 0.8080\n","Epoch 27/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4373 - acc: 0.8023 - val_loss: 0.4326 - val_acc: 0.8083\n","Epoch 28/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4357 - acc: 0.8034 - val_loss: 0.4325 - val_acc: 0.8090\n","Epoch 29/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4365 - acc: 0.8060 - val_loss: 0.4323 - val_acc: 0.8093\n","Epoch 30/100\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.4354 - acc: 0.8067 - val_loss: 0.4323 - val_acc: 0.8087\n","Epoch 31/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.4369 - acc: 0.8037 - val_loss: 0.4322 - val_acc: 0.8093\n","Epoch 32/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.4365 - acc: 0.8054 - val_loss: 0.4321 - val_acc: 0.8103\n","Epoch 33/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.4365 - acc: 0.8059 - val_loss: 0.4321 - val_acc: 0.8097\n","Epoch 34/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.4363 - acc: 0.8070 - val_loss: 0.4320 - val_acc: 0.8100\n","Epoch 35/100\n","7000/7000 [==============================] - 0s 41us/sample - loss: 0.4350 - acc: 0.8096 - val_loss: 0.4320 - val_acc: 0.8097\n","Epoch 36/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.4344 - acc: 0.8067 - val_loss: 0.4320 - val_acc: 0.8100\n","Epoch 37/100\n","7000/7000 [==============================] - 0s 42us/sample - loss: 0.4361 - acc: 0.8079 - val_loss: 0.4320 - val_acc: 0.8100\n","Epoch 38/100\n","7000/7000 [==============================] - 0s 41us/sample - loss: 0.4376 - acc: 0.8057 - val_loss: 0.4319 - val_acc: 0.8103\n","Epoch 39/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.4359 - acc: 0.8063 - val_loss: 0.4320 - val_acc: 0.8113\n","Epoch 40/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.4351 - acc: 0.8084 - val_loss: 0.4320 - val_acc: 0.8107\n","Epoch 41/100\n","7000/7000 [==============================] - 0s 41us/sample - loss: 0.4360 - acc: 0.8069 - val_loss: 0.4319 - val_acc: 0.8110\n","Epoch 42/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.4376 - acc: 0.8054 - val_loss: 0.4320 - val_acc: 0.8110\n","Epoch 43/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.4359 - acc: 0.8080 - val_loss: 0.4319 - val_acc: 0.8107\n","Epoch 44/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.4367 - acc: 0.8050 - val_loss: 0.4319 - val_acc: 0.8107\n","Epoch 45/100\n","7000/7000 [==============================] - 0s 41us/sample - loss: 0.4361 - acc: 0.8064 - val_loss: 0.4319 - val_acc: 0.8110\n","Epoch 46/100\n","7000/7000 [==============================] - 0s 42us/sample - loss: 0.4368 - acc: 0.8069 - val_loss: 0.4319 - val_acc: 0.8103\n","Epoch 47/100\n","7000/7000 [==============================] - 0s 41us/sample - loss: 0.4362 - acc: 0.8080 - val_loss: 0.4319 - val_acc: 0.8103\n","Epoch 48/100\n","7000/7000 [==============================] - 0s 41us/sample - loss: 0.4353 - acc: 0.8084 - val_loss: 0.4319 - val_acc: 0.8103\n","Epoch 49/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.4366 - acc: 0.8060 - val_loss: 0.4319 - val_acc: 0.8103\n","Epoch 50/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.4370 - acc: 0.8084 - val_loss: 0.4319 - val_acc: 0.8103\n","Epoch 51/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.4373 - acc: 0.8070 - val_loss: 0.4319 - val_acc: 0.8107\n","Epoch 52/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.4365 - acc: 0.8067 - val_loss: 0.4318 - val_acc: 0.8103\n","Epoch 53/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.4356 - acc: 0.8079 - val_loss: 0.4319 - val_acc: 0.8107\n","Epoch 54/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.4352 - acc: 0.8096 - val_loss: 0.4320 - val_acc: 0.8110\n","Epoch 55/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.4353 - acc: 0.8043 - val_loss: 0.4319 - val_acc: 0.8113\n","Epoch 56/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.4375 - acc: 0.8047 - val_loss: 0.4320 - val_acc: 0.8110\n","Epoch 57/100\n","7000/7000 [==============================] - 0s 41us/sample - loss: 0.4344 - acc: 0.8091 - val_loss: 0.4320 - val_acc: 0.8110\n","Epoch 58/100\n","7000/7000 [==============================] - 0s 41us/sample - loss: 0.4367 - acc: 0.8089 - val_loss: 0.4320 - val_acc: 0.8107\n","Epoch 59/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.4357 - acc: 0.8054 - val_loss: 0.4320 - val_acc: 0.8107\n","Epoch 60/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.4366 - acc: 0.8079 - val_loss: 0.4320 - val_acc: 0.8107\n","Epoch 61/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.4368 - acc: 0.8083 - val_loss: 0.4320 - val_acc: 0.8110\n","Epoch 62/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.4369 - acc: 0.8063 - val_loss: 0.4320 - val_acc: 0.8107\n","Epoch 63/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.4363 - acc: 0.8060 - val_loss: 0.4320 - val_acc: 0.8110\n","Epoch 64/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.4353 - acc: 0.8071 - val_loss: 0.4319 - val_acc: 0.8110\n","Epoch 65/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.4364 - acc: 0.8071 - val_loss: 0.4320 - val_acc: 0.8107\n","Epoch 66/100\n","7000/7000 [==============================] - 0s 38us/sample - loss: 0.4362 - acc: 0.8071 - val_loss: 0.4320 - val_acc: 0.8113\n","Epoch 67/100\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4358 - acc: 0.8074 - val_loss: 0.4320 - val_acc: 0.8110\n","Epoch 68/100\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.4354 - acc: 0.8070 - val_loss: 0.4320 - val_acc: 0.8107\n","Epoch 69/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4358 - acc: 0.8087 - val_loss: 0.4320 - val_acc: 0.8103\n","Epoch 70/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4349 - acc: 0.8083 - val_loss: 0.4320 - val_acc: 0.8107\n","Epoch 71/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4359 - acc: 0.8047 - val_loss: 0.4319 - val_acc: 0.8107\n","Epoch 72/100\n","7000/7000 [==============================] - 0s 33us/sample - loss: 0.4372 - acc: 0.8094 - val_loss: 0.4319 - val_acc: 0.8107\n","Epoch 73/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4343 - acc: 0.8100 - val_loss: 0.4319 - val_acc: 0.8103\n","Epoch 74/100\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4361 - acc: 0.8087 - val_loss: 0.4319 - val_acc: 0.8107\n","Epoch 75/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4359 - acc: 0.8076 - val_loss: 0.4319 - val_acc: 0.8103\n","Epoch 76/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4356 - acc: 0.8091 - val_loss: 0.4319 - val_acc: 0.8103\n","Epoch 77/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4362 - acc: 0.8084 - val_loss: 0.4319 - val_acc: 0.8107\n","Epoch 78/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4364 - acc: 0.8104 - val_loss: 0.4319 - val_acc: 0.8110\n","Epoch 79/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4361 - acc: 0.8084 - val_loss: 0.4319 - val_acc: 0.8113\n","Epoch 80/100\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4360 - acc: 0.8079 - val_loss: 0.4320 - val_acc: 0.8110\n","Epoch 81/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4361 - acc: 0.8080 - val_loss: 0.4319 - val_acc: 0.8107\n","Epoch 82/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4374 - acc: 0.8069 - val_loss: 0.4319 - val_acc: 0.8113\n","Epoch 83/100\n","7000/7000 [==============================] - 0s 38us/sample - loss: 0.4369 - acc: 0.8059 - val_loss: 0.4319 - val_acc: 0.8113\n","Epoch 84/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4373 - acc: 0.8070 - val_loss: 0.4319 - val_acc: 0.8113\n","Epoch 85/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4379 - acc: 0.8070 - val_loss: 0.4319 - val_acc: 0.8110\n","Epoch 86/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4369 - acc: 0.8057 - val_loss: 0.4319 - val_acc: 0.8107\n","Epoch 87/100\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4357 - acc: 0.8069 - val_loss: 0.4319 - val_acc: 0.8107\n","Epoch 88/100\n","7000/7000 [==============================] - 0s 33us/sample - loss: 0.4369 - acc: 0.8071 - val_loss: 0.4319 - val_acc: 0.8103\n","Epoch 89/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4355 - acc: 0.8084 - val_loss: 0.4320 - val_acc: 0.8103\n","Epoch 90/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4363 - acc: 0.8050 - val_loss: 0.4319 - val_acc: 0.8110\n","Epoch 91/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4368 - acc: 0.8093 - val_loss: 0.4319 - val_acc: 0.8107\n","Epoch 92/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4361 - acc: 0.8091 - val_loss: 0.4319 - val_acc: 0.8100\n","Epoch 93/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4356 - acc: 0.8091 - val_loss: 0.4319 - val_acc: 0.8107\n","Epoch 94/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4351 - acc: 0.8101 - val_loss: 0.4319 - val_acc: 0.8100\n","Epoch 95/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4361 - acc: 0.8094 - val_loss: 0.4320 - val_acc: 0.8100\n","Epoch 96/100\n","7000/7000 [==============================] - 0s 33us/sample - loss: 0.4371 - acc: 0.8057 - val_loss: 0.4319 - val_acc: 0.8107\n","Epoch 97/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4359 - acc: 0.8093 - val_loss: 0.4320 - val_acc: 0.8103\n","Epoch 98/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4362 - acc: 0.8083 - val_loss: 0.4319 - val_acc: 0.8107\n","Epoch 99/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4359 - acc: 0.8074 - val_loss: 0.4319 - val_acc: 0.8107\n","Epoch 100/100\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4353 - acc: 0.8080 - val_loss: 0.4319 - val_acc: 0.8110\n","0.811\n","[[2322   73]\n"," [ 494  111]]\n","              precision    recall  f1-score   support\n","\n","           0       0.82      0.97      0.89      2395\n","           1       0.60      0.18      0.28       605\n","\n","   micro avg       0.81      0.81      0.81      3000\n","   macro avg       0.71      0.58      0.59      3000\n","weighted avg       0.78      0.81      0.77      3000\n","\n"],"name":"stdout"}]},{"metadata":{"id":"dd00oemEKlFq","colab_type":"text"},"cell_type":"markdown","source":["<h4>Optimization</h4>\n","<h4>Neural Network with  1 hidden layer (relu function)</h4>\n","\n","Neural network with 1 hidden layer has only input layer with 11 variables and 1 output/dependant variables. Learning rate of 0.03 is used and batch value of 52 is used\n","\n","Sigmoid function gives values from 0 to 1 for the test set. With 0.5 as threshold, if the predicted value is > 0.5, output will be 1 and if the predicted value < 0.5, output will be 0\n","\n","This Neural Network gives an accuracy of 85.73% and f1-score of 92% (true) and 54% (false) and 84% as weighted average"]},{"metadata":{"id":"ofcqluEhXPsc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":2006},"outputId":"1d9ecd84-e5da-4b0a-d3ce-46058e04f749","executionInfo":{"status":"ok","timestamp":1549915216166,"user_tz":-330,"elapsed":174491,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["#Initialize Sequential model\n","model = tf.keras.models.Sequential()\n","\n","#Reshape data from 2D to 1D\n","model.add(tf.keras.layers.Reshape((11,),input_shape=(11,)))\n","\n","#Normalize the data\n","model.add(tf.keras.layers.BatchNormalization())\n","\n","#Add 1st hidden layer\n","model.add(tf.keras.layers.Dense(10, activation='relu'))\n","\n","#Add OUTPUT layer\n","model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n","\n","#Create optimizer with non-default learning rate\n","sgd_optimizer = tf.keras.optimizers.SGD(lr=0.03)\n","\n","#Compile the model\n","model.compile(optimizer=sgd_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","\n","model.fit(X_train_ss, Y_train, validation_data=(X_test_ss,Y_test), epochs=50, batch_size=52)\n","\n","Y_1_pred = model.predict(X_test_ss)\n","Z_1_pred = to_y_pred_bin(Y_1_pred)\n","\n","print(accuracy_score(Y_test,Z_1_pred))\n","print(confusion_matrix(Y_test,Z_1_pred))\n","print(classification_report(Y_test,Z_1_pred))"],"execution_count":106,"outputs":[{"output_type":"stream","text":["Train on 7000 samples, validate on 3000 samples\n","Epoch 1/50\n","7000/7000 [==============================] - 1s 127us/sample - loss: 0.5471 - acc: 0.7620 - val_loss: 0.5046 - val_acc: 0.7873\n","Epoch 2/50\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4905 - acc: 0.7907 - val_loss: 0.4751 - val_acc: 0.7927\n","Epoch 3/50\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4716 - acc: 0.7923 - val_loss: 0.4605 - val_acc: 0.7933\n","Epoch 4/50\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4598 - acc: 0.7913 - val_loss: 0.4513 - val_acc: 0.7933\n","Epoch 5/50\n","7000/7000 [==============================] - 0s 38us/sample - loss: 0.4513 - acc: 0.7921 - val_loss: 0.4450 - val_acc: 0.7980\n","Epoch 6/50\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4473 - acc: 0.7947 - val_loss: 0.4409 - val_acc: 0.7980\n","Epoch 7/50\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4440 - acc: 0.7956 - val_loss: 0.4372 - val_acc: 0.8000\n","Epoch 8/50\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4416 - acc: 0.8003 - val_loss: 0.4345 - val_acc: 0.8020\n","Epoch 9/50\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4390 - acc: 0.8029 - val_loss: 0.4320 - val_acc: 0.8030\n","Epoch 10/50\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.4359 - acc: 0.8027 - val_loss: 0.4298 - val_acc: 0.8053\n","Epoch 11/50\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4336 - acc: 0.8054 - val_loss: 0.4279 - val_acc: 0.8077\n","Epoch 12/50\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4312 - acc: 0.8043 - val_loss: 0.4258 - val_acc: 0.8093\n","Epoch 13/50\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.4292 - acc: 0.8107 - val_loss: 0.4242 - val_acc: 0.8120\n","Epoch 14/50\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4279 - acc: 0.8087 - val_loss: 0.4219 - val_acc: 0.8150\n","Epoch 15/50\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4274 - acc: 0.8107 - val_loss: 0.4200 - val_acc: 0.8163\n","Epoch 16/50\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.4240 - acc: 0.8109 - val_loss: 0.4182 - val_acc: 0.8190\n","Epoch 17/50\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4222 - acc: 0.8113 - val_loss: 0.4157 - val_acc: 0.8190\n","Epoch 18/50\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4202 - acc: 0.8160 - val_loss: 0.4133 - val_acc: 0.8207\n","Epoch 19/50\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4187 - acc: 0.8153 - val_loss: 0.4107 - val_acc: 0.8220\n","Epoch 20/50\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4141 - acc: 0.8167 - val_loss: 0.4079 - val_acc: 0.8247\n","Epoch 21/50\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4106 - acc: 0.8193 - val_loss: 0.4051 - val_acc: 0.8287\n","Epoch 22/50\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4075 - acc: 0.8200 - val_loss: 0.4012 - val_acc: 0.8297\n","Epoch 23/50\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4024 - acc: 0.8234 - val_loss: 0.3973 - val_acc: 0.8300\n","Epoch 24/50\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4017 - acc: 0.8257 - val_loss: 0.3931 - val_acc: 0.8337\n","Epoch 25/50\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.3939 - acc: 0.8333 - val_loss: 0.3887 - val_acc: 0.8377\n","Epoch 26/50\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3890 - acc: 0.8314 - val_loss: 0.3845 - val_acc: 0.8427\n","Epoch 27/50\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.3863 - acc: 0.8353 - val_loss: 0.3803 - val_acc: 0.8447\n","Epoch 28/50\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.3840 - acc: 0.8384 - val_loss: 0.3765 - val_acc: 0.8490\n","Epoch 29/50\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3793 - acc: 0.8424 - val_loss: 0.3735 - val_acc: 0.8493\n","Epoch 30/50\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.3783 - acc: 0.8414 - val_loss: 0.3707 - val_acc: 0.8510\n","Epoch 31/50\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3741 - acc: 0.8470 - val_loss: 0.3681 - val_acc: 0.8530\n","Epoch 32/50\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3706 - acc: 0.8480 - val_loss: 0.3661 - val_acc: 0.8560\n","Epoch 33/50\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.3710 - acc: 0.8484 - val_loss: 0.3647 - val_acc: 0.8567\n","Epoch 34/50\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3675 - acc: 0.8504 - val_loss: 0.3651 - val_acc: 0.8547\n","Epoch 35/50\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3669 - acc: 0.8516 - val_loss: 0.3631 - val_acc: 0.8550\n","Epoch 36/50\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.3639 - acc: 0.8519 - val_loss: 0.3622 - val_acc: 0.8563\n","Epoch 37/50\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3611 - acc: 0.8553 - val_loss: 0.3616 - val_acc: 0.8577\n","Epoch 38/50\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3627 - acc: 0.8509 - val_loss: 0.3612 - val_acc: 0.8560\n","Epoch 39/50\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3631 - acc: 0.8494 - val_loss: 0.3606 - val_acc: 0.8573\n","Epoch 40/50\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3644 - acc: 0.8477 - val_loss: 0.3597 - val_acc: 0.8573\n","Epoch 41/50\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3583 - acc: 0.8537 - val_loss: 0.3598 - val_acc: 0.8577\n","Epoch 42/50\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.3630 - acc: 0.8511 - val_loss: 0.3584 - val_acc: 0.8583\n","Epoch 43/50\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3634 - acc: 0.8496 - val_loss: 0.3579 - val_acc: 0.8577\n","Epoch 44/50\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3590 - acc: 0.8557 - val_loss: 0.3578 - val_acc: 0.8567\n","Epoch 45/50\n","7000/7000 [==============================] - 0s 38us/sample - loss: 0.3603 - acc: 0.8520 - val_loss: 0.3573 - val_acc: 0.8580\n","Epoch 46/50\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.3608 - acc: 0.8513 - val_loss: 0.3569 - val_acc: 0.8580\n","Epoch 47/50\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.3568 - acc: 0.8533 - val_loss: 0.3563 - val_acc: 0.8580\n","Epoch 48/50\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3579 - acc: 0.8531 - val_loss: 0.3563 - val_acc: 0.8593\n","Epoch 49/50\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.3595 - acc: 0.8534 - val_loss: 0.3557 - val_acc: 0.8587\n","Epoch 50/50\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.3587 - acc: 0.8536 - val_loss: 0.3552 - val_acc: 0.8587\n","0.8586666666666667\n","[[2330   65]\n"," [ 359  246]]\n","              precision    recall  f1-score   support\n","\n","           0       0.87      0.97      0.92      2395\n","           1       0.79      0.41      0.54       605\n","\n","   micro avg       0.86      0.86      0.86      3000\n","   macro avg       0.83      0.69      0.73      3000\n","weighted avg       0.85      0.86      0.84      3000\n","\n"],"name":"stdout"}]},{"metadata":{"id":"a6d3AbAObwCm","colab_type":"text"},"cell_type":"markdown","source":["<h4>Optimization</h4>\n","<h4>Neural Network with 1 hidden layer (tanh function)</h4>\n","\n","Neural network with 1 hidden layer (with tanh activation function) has input layer with 11 variables and 1 output/dependant variables. Learning rate of 0.03 is used and batch value of 52 is used\n","\n","Sigmoid function gives values from 0 to 1 for the test set. With 0.5 as threshold, if the predicted value is > 0.5, output will be 1 and if the predicted value < 0.5, output will be 0\n","\n","This Neural Network gives an accuracy of 85.83% and f1-score of 92% (true) and 54% (false) and 84% as weighted average"]},{"metadata":{"id":"QP7ohr1RYnpw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":3766},"outputId":"571012ae-be87-4c95-8914-74c075885061","executionInfo":{"status":"ok","timestamp":1549915247466,"user_tz":-330,"elapsed":205776,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["#Initialize Sequential model\n","model = tf.keras.models.Sequential()\n","\n","#Reshape data from 2D to 1D\n","model.add(tf.keras.layers.Reshape((11,),input_shape=(11,)))\n","\n","#Normalize the data\n","model.add(tf.keras.layers.BatchNormalization())\n","\n","#Add 1st hidden layer (tanh)\n","model.add(tf.keras.layers.Dense(10, activation='tanh'))\n","\n","#Add OUTPUT layer\n","model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n","\n","#Create optimizer with non-default learning rate\n","sgd_optimizer = tf.keras.optimizers.SGD(lr=0.03)\n","\n","#Compile the model\n","model.compile(optimizer=sgd_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","\n","model.fit(X_train_ss,Y_train,validation_data=(X_test_ss,Y_test),epochs=100,batch_size=52)\n","\n","Y_tan_pred = model.predict(X_test_ss)\n","Z_tan_pred = to_y_pred_bin(Y_tan_pred)\n","\n","print(accuracy_score(Y_test,Z_tan_pred))\n","print(classification_report(Y_test,Z_tan_pred))\n","print(confusion_matrix(Y_test,Z_tan_pred))"],"execution_count":107,"outputs":[{"output_type":"stream","text":["Train on 7000 samples, validate on 3000 samples\n","Epoch 1/100\n","7000/7000 [==============================] - 1s 127us/sample - loss: 0.5797 - acc: 0.7100 - val_loss: 0.4807 - val_acc: 0.7960\n","Epoch 2/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.4640 - acc: 0.7964 - val_loss: 0.4475 - val_acc: 0.8030\n","Epoch 3/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4449 - acc: 0.7997 - val_loss: 0.4365 - val_acc: 0.8080\n","Epoch 4/100\n","7000/7000 [==============================] - 0s 38us/sample - loss: 0.4371 - acc: 0.8007 - val_loss: 0.4309 - val_acc: 0.8077\n","Epoch 5/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4330 - acc: 0.8063 - val_loss: 0.4267 - val_acc: 0.8123\n","Epoch 6/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4300 - acc: 0.8073 - val_loss: 0.4231 - val_acc: 0.8140\n","Epoch 7/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4231 - acc: 0.8090 - val_loss: 0.4195 - val_acc: 0.8177\n","Epoch 8/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4180 - acc: 0.8160 - val_loss: 0.4150 - val_acc: 0.8170\n","Epoch 9/100\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.4166 - acc: 0.8136 - val_loss: 0.4110 - val_acc: 0.8197\n","Epoch 10/100\n","7000/7000 [==============================] - 0s 38us/sample - loss: 0.4119 - acc: 0.8141 - val_loss: 0.4067 - val_acc: 0.8233\n","Epoch 11/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4077 - acc: 0.8184 - val_loss: 0.4030 - val_acc: 0.8230\n","Epoch 12/100\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.4027 - acc: 0.8190 - val_loss: 0.3989 - val_acc: 0.8273\n","Epoch 13/100\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.4014 - acc: 0.8249 - val_loss: 0.3948 - val_acc: 0.8327\n","Epoch 14/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3970 - acc: 0.8281 - val_loss: 0.3914 - val_acc: 0.8360\n","Epoch 15/100\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.3932 - acc: 0.8284 - val_loss: 0.3881 - val_acc: 0.8397\n","Epoch 16/100\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.3907 - acc: 0.8336 - val_loss: 0.3851 - val_acc: 0.8400\n","Epoch 17/100\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.3867 - acc: 0.8377 - val_loss: 0.3820 - val_acc: 0.8417\n","Epoch 18/100\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.3857 - acc: 0.8363 - val_loss: 0.3793 - val_acc: 0.8437\n","Epoch 19/100\n","7000/7000 [==============================] - 0s 38us/sample - loss: 0.3823 - acc: 0.8386 - val_loss: 0.3771 - val_acc: 0.8443\n","Epoch 20/100\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.3803 - acc: 0.8401 - val_loss: 0.3750 - val_acc: 0.8477\n","Epoch 21/100\n","7000/7000 [==============================] - 0s 38us/sample - loss: 0.3789 - acc: 0.8441 - val_loss: 0.3731 - val_acc: 0.8473\n","Epoch 22/100\n","7000/7000 [==============================] - 0s 38us/sample - loss: 0.3764 - acc: 0.8433 - val_loss: 0.3715 - val_acc: 0.8483\n","Epoch 23/100\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.3779 - acc: 0.8443 - val_loss: 0.3702 - val_acc: 0.8480\n","Epoch 24/100\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.3723 - acc: 0.8449 - val_loss: 0.3689 - val_acc: 0.8497\n","Epoch 25/100\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.3750 - acc: 0.8444 - val_loss: 0.3682 - val_acc: 0.8483\n","Epoch 26/100\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.3710 - acc: 0.8479 - val_loss: 0.3668 - val_acc: 0.8483\n","Epoch 27/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3720 - acc: 0.8466 - val_loss: 0.3660 - val_acc: 0.8517\n","Epoch 28/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3695 - acc: 0.8470 - val_loss: 0.3651 - val_acc: 0.8507\n","Epoch 29/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3684 - acc: 0.8474 - val_loss: 0.3640 - val_acc: 0.8503\n","Epoch 30/100\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.3699 - acc: 0.8464 - val_loss: 0.3634 - val_acc: 0.8527\n","Epoch 31/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3687 - acc: 0.8484 - val_loss: 0.3631 - val_acc: 0.8503\n","Epoch 32/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3702 - acc: 0.8444 - val_loss: 0.3623 - val_acc: 0.8520\n","Epoch 33/100\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.3670 - acc: 0.8491 - val_loss: 0.3614 - val_acc: 0.8507\n","Epoch 34/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3650 - acc: 0.8484 - val_loss: 0.3609 - val_acc: 0.8523\n","Epoch 35/100\n","7000/7000 [==============================] - 0s 38us/sample - loss: 0.3670 - acc: 0.8466 - val_loss: 0.3606 - val_acc: 0.8523\n","Epoch 36/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3660 - acc: 0.8501 - val_loss: 0.3598 - val_acc: 0.8517\n","Epoch 37/100\n","7000/7000 [==============================] - 0s 41us/sample - loss: 0.3665 - acc: 0.8479 - val_loss: 0.3593 - val_acc: 0.8503\n","Epoch 38/100\n","7000/7000 [==============================] - 0s 43us/sample - loss: 0.3643 - acc: 0.8513 - val_loss: 0.3593 - val_acc: 0.8507\n","Epoch 39/100\n","7000/7000 [==============================] - 0s 42us/sample - loss: 0.3647 - acc: 0.8499 - val_loss: 0.3586 - val_acc: 0.8537\n","Epoch 40/100\n","7000/7000 [==============================] - 0s 42us/sample - loss: 0.3653 - acc: 0.8494 - val_loss: 0.3585 - val_acc: 0.8510\n","Epoch 41/100\n","7000/7000 [==============================] - 0s 41us/sample - loss: 0.3640 - acc: 0.8476 - val_loss: 0.3580 - val_acc: 0.8513\n","Epoch 42/100\n","7000/7000 [==============================] - 0s 42us/sample - loss: 0.3639 - acc: 0.8490 - val_loss: 0.3573 - val_acc: 0.8543\n","Epoch 43/100\n","7000/7000 [==============================] - 0s 41us/sample - loss: 0.3597 - acc: 0.8527 - val_loss: 0.3569 - val_acc: 0.8520\n","Epoch 44/100\n","7000/7000 [==============================] - 0s 41us/sample - loss: 0.3626 - acc: 0.8490 - val_loss: 0.3571 - val_acc: 0.8530\n","Epoch 45/100\n","7000/7000 [==============================] - 0s 41us/sample - loss: 0.3620 - acc: 0.8503 - val_loss: 0.3572 - val_acc: 0.8537\n","Epoch 46/100\n","7000/7000 [==============================] - 0s 41us/sample - loss: 0.3613 - acc: 0.8503 - val_loss: 0.3572 - val_acc: 0.8487\n","Epoch 47/100\n","7000/7000 [==============================] - 0s 41us/sample - loss: 0.3608 - acc: 0.8513 - val_loss: 0.3559 - val_acc: 0.8550\n","Epoch 48/100\n","7000/7000 [==============================] - 0s 41us/sample - loss: 0.3614 - acc: 0.8497 - val_loss: 0.3560 - val_acc: 0.8543\n","Epoch 49/100\n","7000/7000 [==============================] - 0s 42us/sample - loss: 0.3602 - acc: 0.8507 - val_loss: 0.3561 - val_acc: 0.8503\n","Epoch 50/100\n","7000/7000 [==============================] - 0s 42us/sample - loss: 0.3637 - acc: 0.8487 - val_loss: 0.3555 - val_acc: 0.8557\n","Epoch 51/100\n","7000/7000 [==============================] - 0s 41us/sample - loss: 0.3627 - acc: 0.8506 - val_loss: 0.3549 - val_acc: 0.8557\n","Epoch 52/100\n","7000/7000 [==============================] - 0s 41us/sample - loss: 0.3597 - acc: 0.8511 - val_loss: 0.3550 - val_acc: 0.8553\n","Epoch 53/100\n","7000/7000 [==============================] - 0s 42us/sample - loss: 0.3624 - acc: 0.8484 - val_loss: 0.3545 - val_acc: 0.8553\n","Epoch 54/100\n","7000/7000 [==============================] - 0s 38us/sample - loss: 0.3607 - acc: 0.8503 - val_loss: 0.3544 - val_acc: 0.8540\n","Epoch 55/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3595 - acc: 0.8501 - val_loss: 0.3545 - val_acc: 0.8537\n","Epoch 56/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3616 - acc: 0.8514 - val_loss: 0.3548 - val_acc: 0.8537\n","Epoch 57/100\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.3605 - acc: 0.8514 - val_loss: 0.3538 - val_acc: 0.8550\n","Epoch 58/100\n","7000/7000 [==============================] - 0s 38us/sample - loss: 0.3565 - acc: 0.8529 - val_loss: 0.3546 - val_acc: 0.8520\n","Epoch 59/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3592 - acc: 0.8499 - val_loss: 0.3536 - val_acc: 0.8543\n","Epoch 60/100\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.3588 - acc: 0.8517 - val_loss: 0.3542 - val_acc: 0.8547\n","Epoch 61/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3606 - acc: 0.8503 - val_loss: 0.3534 - val_acc: 0.8557\n","Epoch 62/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3602 - acc: 0.8510 - val_loss: 0.3536 - val_acc: 0.8563\n","Epoch 63/100\n","7000/7000 [==============================] - 0s 38us/sample - loss: 0.3607 - acc: 0.8516 - val_loss: 0.3534 - val_acc: 0.8567\n","Epoch 64/100\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.3590 - acc: 0.8493 - val_loss: 0.3532 - val_acc: 0.8563\n","Epoch 65/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3576 - acc: 0.8504 - val_loss: 0.3532 - val_acc: 0.8560\n","Epoch 66/100\n","7000/7000 [==============================] - 0s 38us/sample - loss: 0.3577 - acc: 0.8486 - val_loss: 0.3531 - val_acc: 0.8560\n","Epoch 67/100\n","7000/7000 [==============================] - 0s 38us/sample - loss: 0.3571 - acc: 0.8526 - val_loss: 0.3534 - val_acc: 0.8557\n","Epoch 68/100\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.3592 - acc: 0.8474 - val_loss: 0.3532 - val_acc: 0.8553\n","Epoch 69/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3573 - acc: 0.8520 - val_loss: 0.3530 - val_acc: 0.8563\n","Epoch 70/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3613 - acc: 0.8487 - val_loss: 0.3528 - val_acc: 0.8560\n","Epoch 71/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3586 - acc: 0.8510 - val_loss: 0.3529 - val_acc: 0.8573\n","Epoch 72/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3554 - acc: 0.8544 - val_loss: 0.3522 - val_acc: 0.8567\n","Epoch 73/100\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.3570 - acc: 0.8510 - val_loss: 0.3523 - val_acc: 0.8573\n","Epoch 74/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3551 - acc: 0.8531 - val_loss: 0.3523 - val_acc: 0.8577\n","Epoch 75/100\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.3563 - acc: 0.8539 - val_loss: 0.3537 - val_acc: 0.8590\n","Epoch 76/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3602 - acc: 0.8520 - val_loss: 0.3519 - val_acc: 0.8567\n","Epoch 77/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3550 - acc: 0.8523 - val_loss: 0.3520 - val_acc: 0.8577\n","Epoch 78/100\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.3582 - acc: 0.8521 - val_loss: 0.3519 - val_acc: 0.8573\n","Epoch 79/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3585 - acc: 0.8497 - val_loss: 0.3520 - val_acc: 0.8573\n","Epoch 80/100\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.3596 - acc: 0.8506 - val_loss: 0.3525 - val_acc: 0.8577\n","Epoch 81/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3563 - acc: 0.8506 - val_loss: 0.3521 - val_acc: 0.8577\n","Epoch 82/100\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.3558 - acc: 0.8544 - val_loss: 0.3531 - val_acc: 0.8590\n","Epoch 83/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3574 - acc: 0.8469 - val_loss: 0.3515 - val_acc: 0.8573\n","Epoch 84/100\n","7000/7000 [==============================] - 0s 38us/sample - loss: 0.3565 - acc: 0.8511 - val_loss: 0.3516 - val_acc: 0.8570\n","Epoch 85/100\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.3569 - acc: 0.8507 - val_loss: 0.3512 - val_acc: 0.8567\n","Epoch 86/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3564 - acc: 0.8501 - val_loss: 0.3515 - val_acc: 0.8580\n","Epoch 87/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3570 - acc: 0.8501 - val_loss: 0.3515 - val_acc: 0.8577\n","Epoch 88/100\n","7000/7000 [==============================] - 0s 38us/sample - loss: 0.3538 - acc: 0.8549 - val_loss: 0.3513 - val_acc: 0.8577\n","Epoch 89/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3557 - acc: 0.8524 - val_loss: 0.3511 - val_acc: 0.8573\n","Epoch 90/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3551 - acc: 0.8499 - val_loss: 0.3510 - val_acc: 0.8563\n","Epoch 91/100\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.3562 - acc: 0.8487 - val_loss: 0.3510 - val_acc: 0.8567\n","Epoch 92/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3547 - acc: 0.8550 - val_loss: 0.3512 - val_acc: 0.8573\n","Epoch 93/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3578 - acc: 0.8517 - val_loss: 0.3513 - val_acc: 0.8570\n","Epoch 94/100\n","7000/7000 [==============================] - 0s 38us/sample - loss: 0.3559 - acc: 0.8541 - val_loss: 0.3509 - val_acc: 0.8573\n","Epoch 95/100\n","7000/7000 [==============================] - 0s 38us/sample - loss: 0.3581 - acc: 0.8526 - val_loss: 0.3507 - val_acc: 0.8570\n","Epoch 96/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3551 - acc: 0.8533 - val_loss: 0.3505 - val_acc: 0.8573\n","Epoch 97/100\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.3552 - acc: 0.8514 - val_loss: 0.3511 - val_acc: 0.8560\n","Epoch 98/100\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.3560 - acc: 0.8527 - val_loss: 0.3511 - val_acc: 0.8577\n","Epoch 99/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3522 - acc: 0.8556 - val_loss: 0.3515 - val_acc: 0.8583\n","Epoch 100/100\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.3553 - acc: 0.8517 - val_loss: 0.3504 - val_acc: 0.8560\n","0.856\n","              precision    recall  f1-score   support\n","\n","           0       0.86      0.97      0.92      2395\n","           1       0.79      0.39      0.52       605\n","\n","   micro avg       0.86      0.86      0.86      3000\n","   macro avg       0.83      0.68      0.72      3000\n","weighted avg       0.85      0.86      0.84      3000\n","\n","[[2332   63]\n"," [ 369  236]]\n"],"name":"stdout"}]},{"metadata":{"id":"fhtlDjyDfGZB","colab_type":"text"},"cell_type":"markdown","source":["<h4>Optimization</h4>\n","<h4>Neural Network with 1 hidden layer (leaky relu function)</h4>\n","\n","Neural network with 1 hidden layer (with leaky relu activation function) has input layer with 11 variables and 1 output/dependant variables. Learning rate of 0.03 is used and batch value of 52 is used\n","\n","Sigmoid function gives values from 0 to 1 for the test set. With 0.5 as threshold, if the predicted value is > 0.5, output will be 1 and if the predicted value < 0.5, output will be 0\n","\n","This Neural Network gives an accuracy of 83.13% and f1-score of 90% (true) and 40% (false) and 80% as weighted average"]},{"metadata":{"id":"WSR7k6XbfE6D","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":5526},"outputId":"ec32b73e-f497-45ff-a2b6-de196654c98a","executionInfo":{"status":"ok","timestamp":1549915290142,"user_tz":-330,"elapsed":248430,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["#Initialize Sequential model\n","model = tf.keras.models.Sequential()\n","\n","#Reshape data from 2D to 1D\n","model.add(tf.keras.layers.Reshape((11,),input_shape=(11,)))\n","\n","#Normalize the data\n","model.add(tf.keras.layers.BatchNormalization())\n","\n","#Add 1st hidden layer (tanh)\n","model.add(tf.keras.layers.LeakyReLU(alpha=0.03))\n","\n","#Add OUTPUT layer\n","model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n","\n","#Create optimizer with non-default learning rate\n","sgd_optimizer = tf.keras.optimizers.SGD(lr=0.03)\n","\n","#Compile the model\n","model.compile(optimizer=sgd_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","\n","model.fit(X_train_ss,Y_train,validation_data=(X_test_ss,Y_test),epochs=150,batch_size=52)\n","\n","Y_prelu_pred = model.predict(X_test_ss)\n","Z_prelu_pred = to_y_pred_bin(Y_prelu_pred)\n","\n","\n","print(accuracy_score(Y_test,Z_prelu_pred))\n","print(classification_report(Y_test,Z_prelu_pred))\n","print(confusion_matrix(Y_test,Z_prelu_pred))"],"execution_count":108,"outputs":[{"output_type":"stream","text":["Train on 7000 samples, validate on 3000 samples\n","Epoch 1/150\n","7000/7000 [==============================] - 1s 128us/sample - loss: 0.5447 - acc: 0.7803 - val_loss: 0.5144 - val_acc: 0.7960\n","Epoch 2/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.5103 - acc: 0.7950 - val_loss: 0.4951 - val_acc: 0.7983\n","Epoch 3/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4961 - acc: 0.7956 - val_loss: 0.4831 - val_acc: 0.7987\n","Epoch 4/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4841 - acc: 0.7974 - val_loss: 0.4728 - val_acc: 0.8020\n","Epoch 5/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4736 - acc: 0.8013 - val_loss: 0.4634 - val_acc: 0.8083\n","Epoch 6/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4655 - acc: 0.8066 - val_loss: 0.4559 - val_acc: 0.8103\n","Epoch 7/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4588 - acc: 0.8054 - val_loss: 0.4511 - val_acc: 0.8103\n","Epoch 8/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4563 - acc: 0.8037 - val_loss: 0.4479 - val_acc: 0.8123\n","Epoch 9/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4539 - acc: 0.8079 - val_loss: 0.4456 - val_acc: 0.8127\n","Epoch 10/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4503 - acc: 0.8081 - val_loss: 0.4438 - val_acc: 0.8140\n","Epoch 11/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4495 - acc: 0.8061 - val_loss: 0.4424 - val_acc: 0.8123\n","Epoch 12/150\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.4480 - acc: 0.8059 - val_loss: 0.4412 - val_acc: 0.8130\n","Epoch 13/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4456 - acc: 0.8081 - val_loss: 0.4401 - val_acc: 0.8137\n","Epoch 14/150\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.4457 - acc: 0.8086 - val_loss: 0.4395 - val_acc: 0.8117\n","Epoch 15/150\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.4463 - acc: 0.8094 - val_loss: 0.4386 - val_acc: 0.8130\n","Epoch 16/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4448 - acc: 0.8071 - val_loss: 0.4378 - val_acc: 0.8123\n","Epoch 17/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4422 - acc: 0.8097 - val_loss: 0.4373 - val_acc: 0.8133\n","Epoch 18/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4429 - acc: 0.8073 - val_loss: 0.4367 - val_acc: 0.8133\n","Epoch 19/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4410 - acc: 0.8100 - val_loss: 0.4360 - val_acc: 0.8127\n","Epoch 20/150\n","7000/7000 [==============================] - 0s 38us/sample - loss: 0.4416 - acc: 0.8091 - val_loss: 0.4354 - val_acc: 0.8103\n","Epoch 21/150\n","7000/7000 [==============================] - 0s 38us/sample - loss: 0.4399 - acc: 0.8069 - val_loss: 0.4352 - val_acc: 0.8137\n","Epoch 22/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4408 - acc: 0.8091 - val_loss: 0.4347 - val_acc: 0.8130\n","Epoch 23/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4412 - acc: 0.8070 - val_loss: 0.4341 - val_acc: 0.8113\n","Epoch 24/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4386 - acc: 0.8104 - val_loss: 0.4337 - val_acc: 0.8130\n","Epoch 25/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4396 - acc: 0.8077 - val_loss: 0.4333 - val_acc: 0.8137\n","Epoch 26/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4380 - acc: 0.8106 - val_loss: 0.4333 - val_acc: 0.8127\n","Epoch 27/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4395 - acc: 0.8091 - val_loss: 0.4325 - val_acc: 0.8127\n","Epoch 28/150\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.4386 - acc: 0.8096 - val_loss: 0.4319 - val_acc: 0.8140\n","Epoch 29/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4370 - acc: 0.8099 - val_loss: 0.4315 - val_acc: 0.8127\n","Epoch 30/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4363 - acc: 0.8109 - val_loss: 0.4310 - val_acc: 0.8137\n","Epoch 31/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4382 - acc: 0.8096 - val_loss: 0.4306 - val_acc: 0.8140\n","Epoch 32/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4357 - acc: 0.8091 - val_loss: 0.4298 - val_acc: 0.8140\n","Epoch 33/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4351 - acc: 0.8131 - val_loss: 0.4287 - val_acc: 0.8160\n","Epoch 34/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4344 - acc: 0.8129 - val_loss: 0.4280 - val_acc: 0.8153\n","Epoch 35/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4338 - acc: 0.8091 - val_loss: 0.4269 - val_acc: 0.8163\n","Epoch 36/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4321 - acc: 0.8129 - val_loss: 0.4257 - val_acc: 0.8177\n","Epoch 37/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4293 - acc: 0.8144 - val_loss: 0.4237 - val_acc: 0.8187\n","Epoch 38/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4289 - acc: 0.8147 - val_loss: 0.4219 - val_acc: 0.8193\n","Epoch 39/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4272 - acc: 0.8160 - val_loss: 0.4202 - val_acc: 0.8190\n","Epoch 40/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4252 - acc: 0.8153 - val_loss: 0.4179 - val_acc: 0.8207\n","Epoch 41/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4243 - acc: 0.8167 - val_loss: 0.4162 - val_acc: 0.8217\n","Epoch 42/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4228 - acc: 0.8176 - val_loss: 0.4149 - val_acc: 0.8227\n","Epoch 43/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4204 - acc: 0.8190 - val_loss: 0.4139 - val_acc: 0.8230\n","Epoch 44/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4210 - acc: 0.8180 - val_loss: 0.4132 - val_acc: 0.8237\n","Epoch 45/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4214 - acc: 0.8210 - val_loss: 0.4125 - val_acc: 0.8240\n","Epoch 46/150\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.4183 - acc: 0.8221 - val_loss: 0.4119 - val_acc: 0.8247\n","Epoch 47/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4187 - acc: 0.8210 - val_loss: 0.4113 - val_acc: 0.8257\n","Epoch 48/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4173 - acc: 0.8189 - val_loss: 0.4111 - val_acc: 0.8267\n","Epoch 49/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4152 - acc: 0.8241 - val_loss: 0.4105 - val_acc: 0.8260\n","Epoch 50/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4190 - acc: 0.8181 - val_loss: 0.4107 - val_acc: 0.8260\n","Epoch 51/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4159 - acc: 0.8207 - val_loss: 0.4108 - val_acc: 0.8267\n","Epoch 52/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4149 - acc: 0.8204 - val_loss: 0.4098 - val_acc: 0.8277\n","Epoch 53/150\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.4150 - acc: 0.8233 - val_loss: 0.4097 - val_acc: 0.8280\n","Epoch 54/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4155 - acc: 0.8221 - val_loss: 0.4096 - val_acc: 0.8290\n","Epoch 55/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4157 - acc: 0.8239 - val_loss: 0.4094 - val_acc: 0.8290\n","Epoch 56/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4138 - acc: 0.8241 - val_loss: 0.4094 - val_acc: 0.8280\n","Epoch 57/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4127 - acc: 0.8244 - val_loss: 0.4095 - val_acc: 0.8267\n","Epoch 58/150\n","7000/7000 [==============================] - 0s 38us/sample - loss: 0.4150 - acc: 0.8243 - val_loss: 0.4090 - val_acc: 0.8283\n","Epoch 59/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4137 - acc: 0.8247 - val_loss: 0.4095 - val_acc: 0.8270\n","Epoch 60/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4136 - acc: 0.8220 - val_loss: 0.4088 - val_acc: 0.8287\n","Epoch 61/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4135 - acc: 0.8234 - val_loss: 0.4087 - val_acc: 0.8293\n","Epoch 62/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4142 - acc: 0.8256 - val_loss: 0.4086 - val_acc: 0.8293\n","Epoch 63/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4126 - acc: 0.8269 - val_loss: 0.4087 - val_acc: 0.8287\n","Epoch 64/150\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.4103 - acc: 0.8253 - val_loss: 0.4087 - val_acc: 0.8287\n","Epoch 65/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4140 - acc: 0.8226 - val_loss: 0.4086 - val_acc: 0.8290\n","Epoch 66/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4128 - acc: 0.8250 - val_loss: 0.4090 - val_acc: 0.8270\n","Epoch 67/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4121 - acc: 0.8269 - val_loss: 0.4086 - val_acc: 0.8283\n","Epoch 68/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4134 - acc: 0.8266 - val_loss: 0.4085 - val_acc: 0.8287\n","Epoch 69/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4137 - acc: 0.8244 - val_loss: 0.4085 - val_acc: 0.8290\n","Epoch 70/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4117 - acc: 0.8249 - val_loss: 0.4084 - val_acc: 0.8287\n","Epoch 71/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4120 - acc: 0.8250 - val_loss: 0.4086 - val_acc: 0.8280\n","Epoch 72/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4127 - acc: 0.8234 - val_loss: 0.4087 - val_acc: 0.8277\n","Epoch 73/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4116 - acc: 0.8256 - val_loss: 0.4085 - val_acc: 0.8287\n","Epoch 74/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4109 - acc: 0.8249 - val_loss: 0.4084 - val_acc: 0.8290\n","Epoch 75/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4112 - acc: 0.8250 - val_loss: 0.4084 - val_acc: 0.8287\n","Epoch 76/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4123 - acc: 0.8253 - val_loss: 0.4091 - val_acc: 0.8267\n","Epoch 77/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4112 - acc: 0.8243 - val_loss: 0.4085 - val_acc: 0.8287\n","Epoch 78/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4115 - acc: 0.8229 - val_loss: 0.4085 - val_acc: 0.8290\n","Epoch 79/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4124 - acc: 0.8261 - val_loss: 0.4086 - val_acc: 0.8280\n","Epoch 80/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4138 - acc: 0.8254 - val_loss: 0.4086 - val_acc: 0.8277\n","Epoch 81/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4112 - acc: 0.8247 - val_loss: 0.4084 - val_acc: 0.8290\n","Epoch 82/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4136 - acc: 0.8236 - val_loss: 0.4083 - val_acc: 0.8297\n","Epoch 83/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4114 - acc: 0.8250 - val_loss: 0.4087 - val_acc: 0.8303\n","Epoch 84/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4106 - acc: 0.8239 - val_loss: 0.4091 - val_acc: 0.8280\n","Epoch 85/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4120 - acc: 0.8244 - val_loss: 0.4083 - val_acc: 0.8293\n","Epoch 86/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4119 - acc: 0.8243 - val_loss: 0.4085 - val_acc: 0.8293\n","Epoch 87/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4103 - acc: 0.8256 - val_loss: 0.4086 - val_acc: 0.8283\n","Epoch 88/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4123 - acc: 0.8241 - val_loss: 0.4084 - val_acc: 0.8300\n","Epoch 89/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4117 - acc: 0.8241 - val_loss: 0.4084 - val_acc: 0.8300\n","Epoch 90/150\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.4116 - acc: 0.8253 - val_loss: 0.4084 - val_acc: 0.8290\n","Epoch 91/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4105 - acc: 0.8236 - val_loss: 0.4083 - val_acc: 0.8293\n","Epoch 92/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4108 - acc: 0.8260 - val_loss: 0.4089 - val_acc: 0.8273\n","Epoch 93/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4118 - acc: 0.8247 - val_loss: 0.4084 - val_acc: 0.8287\n","Epoch 94/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4114 - acc: 0.8259 - val_loss: 0.4082 - val_acc: 0.8300\n","Epoch 95/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4111 - acc: 0.8250 - val_loss: 0.4083 - val_acc: 0.8297\n","Epoch 96/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4108 - acc: 0.8256 - val_loss: 0.4084 - val_acc: 0.8287\n","Epoch 97/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4105 - acc: 0.8224 - val_loss: 0.4083 - val_acc: 0.8307\n","Epoch 98/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4130 - acc: 0.8213 - val_loss: 0.4086 - val_acc: 0.8283\n","Epoch 99/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4124 - acc: 0.8251 - val_loss: 0.4082 - val_acc: 0.8293\n","Epoch 100/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4128 - acc: 0.8231 - val_loss: 0.4084 - val_acc: 0.8283\n","Epoch 101/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4101 - acc: 0.8254 - val_loss: 0.4082 - val_acc: 0.8303\n","Epoch 102/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4098 - acc: 0.8260 - val_loss: 0.4087 - val_acc: 0.8273\n","Epoch 103/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4127 - acc: 0.8246 - val_loss: 0.4082 - val_acc: 0.8307\n","Epoch 104/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4119 - acc: 0.8254 - val_loss: 0.4084 - val_acc: 0.8290\n","Epoch 105/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4122 - acc: 0.8221 - val_loss: 0.4083 - val_acc: 0.8303\n","Epoch 106/150\n","7000/7000 [==============================] - 0s 38us/sample - loss: 0.4097 - acc: 0.8241 - val_loss: 0.4085 - val_acc: 0.8287\n","Epoch 107/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4117 - acc: 0.8239 - val_loss: 0.4087 - val_acc: 0.8277\n","Epoch 108/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4120 - acc: 0.8261 - val_loss: 0.4080 - val_acc: 0.8297\n","Epoch 109/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4116 - acc: 0.8241 - val_loss: 0.4084 - val_acc: 0.8290\n","Epoch 110/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4095 - acc: 0.8250 - val_loss: 0.4081 - val_acc: 0.8307\n","Epoch 111/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4110 - acc: 0.8257 - val_loss: 0.4081 - val_acc: 0.8317\n","Epoch 112/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4121 - acc: 0.8261 - val_loss: 0.4081 - val_acc: 0.8307\n","Epoch 113/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4111 - acc: 0.8261 - val_loss: 0.4085 - val_acc: 0.8280\n","Epoch 114/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4124 - acc: 0.8261 - val_loss: 0.4083 - val_acc: 0.8290\n","Epoch 115/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4100 - acc: 0.8257 - val_loss: 0.4080 - val_acc: 0.8313\n","Epoch 116/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4122 - acc: 0.8266 - val_loss: 0.4081 - val_acc: 0.8307\n","Epoch 117/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4109 - acc: 0.8239 - val_loss: 0.4080 - val_acc: 0.8307\n","Epoch 118/150\n","7000/7000 [==============================] - 0s 34us/sample - loss: 0.4107 - acc: 0.8250 - val_loss: 0.4083 - val_acc: 0.8290\n","Epoch 119/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4109 - acc: 0.8250 - val_loss: 0.4081 - val_acc: 0.8300\n","Epoch 120/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4114 - acc: 0.8251 - val_loss: 0.4085 - val_acc: 0.8273\n","Epoch 121/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4108 - acc: 0.8250 - val_loss: 0.4081 - val_acc: 0.8303\n","Epoch 122/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4094 - acc: 0.8253 - val_loss: 0.4082 - val_acc: 0.8293\n","Epoch 123/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4102 - acc: 0.8266 - val_loss: 0.4082 - val_acc: 0.8297\n","Epoch 124/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4124 - acc: 0.8250 - val_loss: 0.4080 - val_acc: 0.8303\n","Epoch 125/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4112 - acc: 0.8260 - val_loss: 0.4079 - val_acc: 0.8303\n","Epoch 126/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4111 - acc: 0.8260 - val_loss: 0.4079 - val_acc: 0.8303\n","Epoch 127/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4116 - acc: 0.8251 - val_loss: 0.4079 - val_acc: 0.8300\n","Epoch 128/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4111 - acc: 0.8249 - val_loss: 0.4078 - val_acc: 0.8300\n","Epoch 129/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4100 - acc: 0.8240 - val_loss: 0.4078 - val_acc: 0.8300\n","Epoch 130/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4110 - acc: 0.8260 - val_loss: 0.4078 - val_acc: 0.8300\n","Epoch 131/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4142 - acc: 0.8240 - val_loss: 0.4078 - val_acc: 0.8303\n","Epoch 132/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4100 - acc: 0.8254 - val_loss: 0.4079 - val_acc: 0.8300\n","Epoch 133/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4098 - acc: 0.8270 - val_loss: 0.4083 - val_acc: 0.8260\n","Epoch 134/150\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.4104 - acc: 0.8247 - val_loss: 0.4078 - val_acc: 0.8300\n","Epoch 135/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4100 - acc: 0.8229 - val_loss: 0.4077 - val_acc: 0.8320\n","Epoch 136/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4111 - acc: 0.8247 - val_loss: 0.4077 - val_acc: 0.8317\n","Epoch 137/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4115 - acc: 0.8236 - val_loss: 0.4077 - val_acc: 0.8313\n","Epoch 138/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4092 - acc: 0.8263 - val_loss: 0.4078 - val_acc: 0.8317\n","Epoch 139/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4101 - acc: 0.8277 - val_loss: 0.4084 - val_acc: 0.8260\n","Epoch 140/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4089 - acc: 0.8270 - val_loss: 0.4077 - val_acc: 0.8310\n","Epoch 141/150\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.4113 - acc: 0.8240 - val_loss: 0.4077 - val_acc: 0.8320\n","Epoch 142/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4099 - acc: 0.8250 - val_loss: 0.4077 - val_acc: 0.8310\n","Epoch 143/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4111 - acc: 0.8240 - val_loss: 0.4077 - val_acc: 0.8307\n","Epoch 144/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4097 - acc: 0.8259 - val_loss: 0.4079 - val_acc: 0.8303\n","Epoch 145/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4114 - acc: 0.8253 - val_loss: 0.4076 - val_acc: 0.8323\n","Epoch 146/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4118 - acc: 0.8243 - val_loss: 0.4076 - val_acc: 0.8317\n","Epoch 147/150\n","7000/7000 [==============================] - 0s 35us/sample - loss: 0.4109 - acc: 0.8254 - val_loss: 0.4076 - val_acc: 0.8317\n","Epoch 148/150\n","7000/7000 [==============================] - 0s 37us/sample - loss: 0.4104 - acc: 0.8266 - val_loss: 0.4077 - val_acc: 0.8300\n","Epoch 149/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4122 - acc: 0.8236 - val_loss: 0.4076 - val_acc: 0.8323\n","Epoch 150/150\n","7000/7000 [==============================] - 0s 36us/sample - loss: 0.4110 - acc: 0.8249 - val_loss: 0.4076 - val_acc: 0.8317\n","0.8316666666666667\n","              precision    recall  f1-score   support\n","\n","           0       0.84      0.98      0.90      2395\n","           1       0.73      0.26      0.39       605\n","\n","   micro avg       0.83      0.83      0.83      3000\n","   macro avg       0.78      0.62      0.64      3000\n","weighted avg       0.82      0.83      0.80      3000\n","\n","[[2336   59]\n"," [ 446  159]]\n"],"name":"stdout"}]},{"metadata":{"id":"rh7wPQyObeyu","colab_type":"text"},"cell_type":"markdown","source":["<h4>Optimization</h4>\n","<h4>Neural Network with 2 hidden layer</h4>\n","\n","Neural network with 2 hidden layers has input layer with 11 variables and 1 output/dependant variables. Learning rate of 0.03 is used and batch value of 52 is used\n","\n","Sigmoid function gives values from 0 to 1 for the test set. With 0.5 as threshold, if the predicted value is > 0.5, output will be 1 and if the predicted value < 0.5, output will be 0\n","\n","This Neural Network gives an accuracy of 85.73% and f1-score of 92% (true) and 54% (false) and 84% as weighted average"]},{"metadata":{"id":"Ns7tSWfx-0JC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":4082},"outputId":"3da69e4b-c0f0-4c78-cc91-68f82b413228","executionInfo":{"status":"ok","timestamp":1549915323235,"user_tz":-330,"elapsed":281502,"user":{"displayName":"chenthur murugan","photoUrl":"","userId":"10011307527226996772"}}},"cell_type":"code","source":["#Initialize Sequential model\n","model = tf.keras.models.Sequential()\n","\n","#Reshape data from 2D to 1D \n","model.add(tf.keras.layers.Reshape((11,),input_shape=(11,)))\n","\n","#Normalize the data\n","model.add(tf.keras.layers.BatchNormalization())\n","\n","#Add 1st hidden layer\n","model.add(tf.keras.layers.Dense(10, activation='relu'))##use sigmoid instead of relu for this residency\n","\n","#Add 2nd hidden layer\n","model.add(tf.keras.layers.Dense(10, activation='tanh'))\n","\n","#Add OUTPUT layer\n","model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n","\n","#Create optimizer with non-default learning rate\n","sgd_optimizer = tf.keras.optimizers.SGD(lr=0.03)\n","\n","#Compile the model\n","model.compile(optimizer=sgd_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n","\n","print(model.summary())\n","\n","model.fit(X_train_ss,Y_train,          \n","          validation_data=(X_test_ss,Y_test),\n","          epochs=100,\n","          batch_size=52)\n","\n","Y_2_pred = model.predict(X_test_ss)\n","Z_2_pred = to_y_pred_bin(Y_2_pred)\n","\n","print(accuracy_score(Y_test,Z_2_pred))\n","print(confusion_matrix(Y_test,Z_2_pred))\n","print(classification_report(Y_test,Z_2_pred))"],"execution_count":109,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","reshape_23 (Reshape)         (None, 11)                0         \n","_________________________________________________________________\n","batch_normalization_v1_23 (B (None, 11)                44        \n","_________________________________________________________________\n","dense_33 (Dense)             (None, 10)                120       \n","_________________________________________________________________\n","dense_34 (Dense)             (None, 10)                110       \n","_________________________________________________________________\n","dense_35 (Dense)             (None, 1)                 11        \n","=================================================================\n","Total params: 285\n","Trainable params: 263\n","Non-trainable params: 22\n","_________________________________________________________________\n","None\n","Train on 7000 samples, validate on 3000 samples\n","Epoch 1/100\n","7000/7000 [==============================] - 1s 141us/sample - loss: 0.5184 - acc: 0.7734 - val_loss: 0.4709 - val_acc: 0.7990\n","Epoch 2/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.4643 - acc: 0.8019 - val_loss: 0.4480 - val_acc: 0.8100\n","Epoch 3/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.4448 - acc: 0.8087 - val_loss: 0.4347 - val_acc: 0.8150\n","Epoch 4/100\n","7000/7000 [==============================] - 0s 38us/sample - loss: 0.4346 - acc: 0.8143 - val_loss: 0.4268 - val_acc: 0.8160\n","Epoch 5/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.4300 - acc: 0.8181 - val_loss: 0.4219 - val_acc: 0.8207\n","Epoch 6/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.4259 - acc: 0.8190 - val_loss: 0.4184 - val_acc: 0.8240\n","Epoch 7/100\n","7000/7000 [==============================] - 0s 38us/sample - loss: 0.4208 - acc: 0.8247 - val_loss: 0.4155 - val_acc: 0.8297\n","Epoch 8/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.4202 - acc: 0.8224 - val_loss: 0.4126 - val_acc: 0.8347\n","Epoch 9/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.4130 - acc: 0.8284 - val_loss: 0.4091 - val_acc: 0.8377\n","Epoch 10/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.4108 - acc: 0.8329 - val_loss: 0.4050 - val_acc: 0.8397\n","Epoch 11/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.4072 - acc: 0.8326 - val_loss: 0.4002 - val_acc: 0.8380\n","Epoch 12/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.4011 - acc: 0.8341 - val_loss: 0.3945 - val_acc: 0.8420\n","Epoch 13/100\n","7000/7000 [==============================] - 0s 38us/sample - loss: 0.3933 - acc: 0.8367 - val_loss: 0.3886 - val_acc: 0.8423\n","Epoch 14/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.3875 - acc: 0.8414 - val_loss: 0.3822 - val_acc: 0.8480\n","Epoch 15/100\n","7000/7000 [==============================] - 0s 41us/sample - loss: 0.3813 - acc: 0.8396 - val_loss: 0.3770 - val_acc: 0.8467\n","Epoch 16/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.3763 - acc: 0.8466 - val_loss: 0.3722 - val_acc: 0.8507\n","Epoch 17/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3710 - acc: 0.8466 - val_loss: 0.3681 - val_acc: 0.8520\n","Epoch 18/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.3679 - acc: 0.8489 - val_loss: 0.3648 - val_acc: 0.8530\n","Epoch 19/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.3619 - acc: 0.8519 - val_loss: 0.3624 - val_acc: 0.8533\n","Epoch 20/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3624 - acc: 0.8503 - val_loss: 0.3614 - val_acc: 0.8510\n","Epoch 21/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3629 - acc: 0.8503 - val_loss: 0.3599 - val_acc: 0.8527\n","Epoch 22/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3613 - acc: 0.8490 - val_loss: 0.3586 - val_acc: 0.8557\n","Epoch 23/100\n","7000/7000 [==============================] - 0s 38us/sample - loss: 0.3566 - acc: 0.8540 - val_loss: 0.3592 - val_acc: 0.8490\n","Epoch 24/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3596 - acc: 0.8544 - val_loss: 0.3569 - val_acc: 0.8560\n","Epoch 25/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.3566 - acc: 0.8547 - val_loss: 0.3566 - val_acc: 0.8567\n","Epoch 26/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.3571 - acc: 0.8539 - val_loss: 0.3559 - val_acc: 0.8567\n","Epoch 27/100\n","7000/7000 [==============================] - 0s 38us/sample - loss: 0.3544 - acc: 0.8560 - val_loss: 0.3553 - val_acc: 0.8560\n","Epoch 28/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3544 - acc: 0.8549 - val_loss: 0.3555 - val_acc: 0.8563\n","Epoch 29/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3572 - acc: 0.8514 - val_loss: 0.3547 - val_acc: 0.8560\n","Epoch 30/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3560 - acc: 0.8526 - val_loss: 0.3550 - val_acc: 0.8573\n","Epoch 31/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3545 - acc: 0.8554 - val_loss: 0.3541 - val_acc: 0.8580\n","Epoch 32/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3563 - acc: 0.8540 - val_loss: 0.3541 - val_acc: 0.8567\n","Epoch 33/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.3542 - acc: 0.8513 - val_loss: 0.3538 - val_acc: 0.8567\n","Epoch 34/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3548 - acc: 0.8526 - val_loss: 0.3554 - val_acc: 0.8527\n","Epoch 35/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3520 - acc: 0.8540 - val_loss: 0.3536 - val_acc: 0.8537\n","Epoch 36/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3542 - acc: 0.8540 - val_loss: 0.3526 - val_acc: 0.8583\n","Epoch 37/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3539 - acc: 0.8544 - val_loss: 0.3527 - val_acc: 0.8587\n","Epoch 38/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.3542 - acc: 0.8560 - val_loss: 0.3527 - val_acc: 0.8543\n","Epoch 39/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3525 - acc: 0.8547 - val_loss: 0.3516 - val_acc: 0.8590\n","Epoch 40/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3529 - acc: 0.8563 - val_loss: 0.3526 - val_acc: 0.8547\n","Epoch 41/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.3527 - acc: 0.8536 - val_loss: 0.3516 - val_acc: 0.8573\n","Epoch 42/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3515 - acc: 0.8546 - val_loss: 0.3523 - val_acc: 0.8543\n","Epoch 43/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3529 - acc: 0.8510 - val_loss: 0.3512 - val_acc: 0.8553\n","Epoch 44/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3515 - acc: 0.8543 - val_loss: 0.3508 - val_acc: 0.8550\n","Epoch 45/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3539 - acc: 0.8524 - val_loss: 0.3505 - val_acc: 0.8557\n","Epoch 46/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.3495 - acc: 0.8567 - val_loss: 0.3499 - val_acc: 0.8570\n","Epoch 47/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3476 - acc: 0.8587 - val_loss: 0.3498 - val_acc: 0.8567\n","Epoch 48/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3500 - acc: 0.8550 - val_loss: 0.3508 - val_acc: 0.8537\n","Epoch 49/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3481 - acc: 0.8564 - val_loss: 0.3494 - val_acc: 0.8557\n","Epoch 50/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3480 - acc: 0.8576 - val_loss: 0.3519 - val_acc: 0.8550\n","Epoch 51/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.3504 - acc: 0.8553 - val_loss: 0.3514 - val_acc: 0.8560\n","Epoch 52/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3474 - acc: 0.8570 - val_loss: 0.3489 - val_acc: 0.8543\n","Epoch 53/100\n","7000/7000 [==============================] - 0s 38us/sample - loss: 0.3505 - acc: 0.8553 - val_loss: 0.3500 - val_acc: 0.8547\n","Epoch 54/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3486 - acc: 0.8553 - val_loss: 0.3493 - val_acc: 0.8553\n","Epoch 55/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3447 - acc: 0.8591 - val_loss: 0.3497 - val_acc: 0.8557\n","Epoch 56/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.3484 - acc: 0.8553 - val_loss: 0.3499 - val_acc: 0.8543\n","Epoch 57/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.3442 - acc: 0.8583 - val_loss: 0.3479 - val_acc: 0.8570\n","Epoch 58/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.3499 - acc: 0.8560 - val_loss: 0.3476 - val_acc: 0.8560\n","Epoch 59/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.3488 - acc: 0.8564 - val_loss: 0.3472 - val_acc: 0.8567\n","Epoch 60/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.3467 - acc: 0.8549 - val_loss: 0.3479 - val_acc: 0.8553\n","Epoch 61/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3460 - acc: 0.8566 - val_loss: 0.3467 - val_acc: 0.8560\n","Epoch 62/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3491 - acc: 0.8547 - val_loss: 0.3487 - val_acc: 0.8533\n","Epoch 63/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3484 - acc: 0.8551 - val_loss: 0.3466 - val_acc: 0.8577\n","Epoch 64/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3448 - acc: 0.8583 - val_loss: 0.3464 - val_acc: 0.8560\n","Epoch 65/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3464 - acc: 0.8570 - val_loss: 0.3463 - val_acc: 0.8553\n","Epoch 66/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3459 - acc: 0.8584 - val_loss: 0.3463 - val_acc: 0.8557\n","Epoch 67/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3462 - acc: 0.8570 - val_loss: 0.3467 - val_acc: 0.8553\n","Epoch 68/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3468 - acc: 0.8591 - val_loss: 0.3466 - val_acc: 0.8553\n","Epoch 69/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.3464 - acc: 0.8599 - val_loss: 0.3483 - val_acc: 0.8527\n","Epoch 70/100\n","7000/7000 [==============================] - 0s 38us/sample - loss: 0.3450 - acc: 0.8581 - val_loss: 0.3457 - val_acc: 0.8570\n","Epoch 71/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3446 - acc: 0.8616 - val_loss: 0.3469 - val_acc: 0.8537\n","Epoch 72/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.3460 - acc: 0.8609 - val_loss: 0.3457 - val_acc: 0.8573\n","Epoch 73/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.3432 - acc: 0.8599 - val_loss: 0.3454 - val_acc: 0.8550\n","Epoch 74/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3449 - acc: 0.8583 - val_loss: 0.3463 - val_acc: 0.8567\n","Epoch 75/100\n","7000/7000 [==============================] - 0s 43us/sample - loss: 0.3456 - acc: 0.8573 - val_loss: 0.3462 - val_acc: 0.8557\n","Epoch 76/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3448 - acc: 0.8584 - val_loss: 0.3450 - val_acc: 0.8573\n","Epoch 77/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3469 - acc: 0.8559 - val_loss: 0.3449 - val_acc: 0.8590\n","Epoch 78/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3435 - acc: 0.8579 - val_loss: 0.3454 - val_acc: 0.8567\n","Epoch 79/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.3436 - acc: 0.8617 - val_loss: 0.3477 - val_acc: 0.8547\n","Epoch 80/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3455 - acc: 0.8576 - val_loss: 0.3441 - val_acc: 0.8583\n","Epoch 81/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3436 - acc: 0.8591 - val_loss: 0.3439 - val_acc: 0.8567\n","Epoch 82/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3424 - acc: 0.8604 - val_loss: 0.3451 - val_acc: 0.8583\n","Epoch 83/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.3458 - acc: 0.8569 - val_loss: 0.3443 - val_acc: 0.8590\n","Epoch 84/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.3460 - acc: 0.8593 - val_loss: 0.3438 - val_acc: 0.8587\n","Epoch 85/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3457 - acc: 0.8574 - val_loss: 0.3436 - val_acc: 0.8620\n","Epoch 86/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3442 - acc: 0.8549 - val_loss: 0.3429 - val_acc: 0.8590\n","Epoch 87/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.3436 - acc: 0.8607 - val_loss: 0.3438 - val_acc: 0.8560\n","Epoch 88/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3433 - acc: 0.8567 - val_loss: 0.3429 - val_acc: 0.8573\n","Epoch 89/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3447 - acc: 0.8561 - val_loss: 0.3429 - val_acc: 0.8580\n","Epoch 90/100\n","7000/7000 [==============================] - 0s 39us/sample - loss: 0.3433 - acc: 0.8594 - val_loss: 0.3447 - val_acc: 0.8580\n","Epoch 91/100\n","7000/7000 [==============================] - 0s 40us/sample - loss: 0.3418 - acc: 0.8600 - val_loss: 0.3430 - val_acc: 0.8583\n","Epoch 92/100\n","7000/7000 [==============================] - 0s 41us/sample - loss: 0.3446 - acc: 0.8589 - val_loss: 0.3427 - val_acc: 0.8590\n","Epoch 93/100\n","7000/7000 [==============================] - 0s 41us/sample - loss: 0.3413 - acc: 0.8606 - val_loss: 0.3424 - val_acc: 0.8607\n","Epoch 94/100\n","7000/7000 [==============================] - 0s 43us/sample - loss: 0.3417 - acc: 0.8547 - val_loss: 0.3421 - val_acc: 0.8600\n","Epoch 95/100\n","7000/7000 [==============================] - 0s 43us/sample - loss: 0.3431 - acc: 0.8586 - val_loss: 0.3423 - val_acc: 0.8583\n","Epoch 96/100\n","7000/7000 [==============================] - 0s 43us/sample - loss: 0.3430 - acc: 0.8603 - val_loss: 0.3432 - val_acc: 0.8590\n","Epoch 97/100\n","7000/7000 [==============================] - 0s 42us/sample - loss: 0.3436 - acc: 0.8597 - val_loss: 0.3417 - val_acc: 0.8603\n","Epoch 98/100\n","7000/7000 [==============================] - 0s 42us/sample - loss: 0.3422 - acc: 0.8581 - val_loss: 0.3472 - val_acc: 0.8550\n","Epoch 99/100\n","7000/7000 [==============================] - 0s 42us/sample - loss: 0.3438 - acc: 0.8556 - val_loss: 0.3422 - val_acc: 0.8600\n","Epoch 100/100\n","7000/7000 [==============================] - 0s 43us/sample - loss: 0.3403 - acc: 0.8569 - val_loss: 0.3448 - val_acc: 0.8603\n","0.8603333333333333\n","[[2337   58]\n"," [ 361  244]]\n","              precision    recall  f1-score   support\n","\n","           0       0.87      0.98      0.92      2395\n","           1       0.81      0.40      0.54       605\n","\n","   micro avg       0.86      0.86      0.86      3000\n","   macro avg       0.84      0.69      0.73      3000\n","weighted avg       0.85      0.86      0.84      3000\n","\n"],"name":"stdout"}]},{"metadata":{"id":"CrIbjhq6-BVq","colab_type":"text"},"cell_type":"markdown","source":["<h3>Summary</h3>\n","\n","In this exercise, bank data were used to predict whether a user will exit or not. Bank data had 2 serial numbers which can be dropped as it does not seem to contribute to the prediction whether the customer will exit or not. The dataset also had 3 object datatypes, hence it was label encoded to convert to numerical values. Independant variables are formed with 11 datapoints. Dependant variable is formed with 1 datapoint. Dataset is split into train and test dataset for both independant and dependant variables. StandardScalar is applied to normalize the data\n","\n","*   Logistic Regression is done using scikit to find out the initial accuracy (81%) along with f1-score (positive) (89%) and f1-score (negative) (28%) and overall weighted avg is 77%\n","*   NeuralNetwork is developed with no hidden layer with default learning rate and learning rate = 0.03 with batch size = 11 and 52 gives the same accuracy as the one given by logistic regression\n","*   <b><u>Optimization</u></b>\n","*   NeuralNetwork is developed with one hidden layer of 10 nodes with relu activation function with learning rate = 0.03 and with batch size = 52 gives an accuracy of 85.14% along with f1-score(positive) = 91% and f1-score (negative) = 53% and overall weighted avg is 84%\n","*   NeuralNetwork is developed with one hidden layer of 10 nodes with tanh activation function with learning rate = 0.03 and with batch size = 52 gives an accuracy of 86% along with f1-score(positive) = 92% and f1-score (negative) = 54% and overall weighted avg is 84%\n","*   NeuralNetwork is developed with one hidden layer of 10 nodes with leaky relu activation function with learning rate = 0.03 and with batch size = 52 gives an accuracy of 83.13% along with f1-score(positive) = 90% and f1-score (negative) = 40% and overall weighted avg is 80% \n","*   NeuralNetwork is developed with two hidden layers of 10 nodes each one with relu and one with tanh activation function with learning rate = 0.03 and with batch size = 52 gives an accuracy of 85.6% along with f1-score(positive) = 91% and f1-score(negative) = 55% and overall weighted avg is 84%\n","\n","\n","\n"]},{"metadata":{"id":"kl4SAXZnAzX7","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"i8swu2S9A1hN","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"3acGnT7dA3s9","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"GtIhkXhCA6Bu","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"Y6Q3W1EibWhj","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}